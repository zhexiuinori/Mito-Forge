"""
æ³¨é‡Šæ™ºèƒ½ä½“ - åŸºäº AI çš„åŸºå› æ³¨é‡Šåˆ†æ
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional

from .base_agent import BaseAgent
from .types import AgentStatus, StageResult, AgentCapability
from .exceptions import AnnotationFailedError, ToolNotFoundError
from ...utils.logging import get_logger

logger = get_logger(__name__)

# æ³¨é‡Šåˆ†æç³»ç»Ÿæç¤ºè¯
ANNOTATION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦åŸºå› æ³¨é‡Šä¸“å®¶ï¼Œè´Ÿè´£åˆ†æåŸºå› æ³¨é‡Šç»“æœå¹¶æä¾›è´¨é‡è¯„ä¼°ã€‚

ä½ çš„èŒè´£ï¼š
1. è¯„ä¼°æ³¨é‡Šå®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼ˆåŸºå› æ•°é‡ã€ç±»å‹ã€åŠŸèƒ½ç­‰ï¼‰
2. è¯†åˆ«æ³¨é‡Šé—®é¢˜ï¼ˆç¼ºå¤±åŸºå› ã€é”™è¯¯é¢„æµ‹ã€é‡å ç­‰ï¼‰
3. æ¨èæ³¨é‡Šä¼˜åŒ–ç­–ç•¥å’Œæ‰‹åŠ¨æ ¡æ­£å»ºè®®
4. è¯„ä¼°æ³¨é‡Šç»“æœçš„ç”Ÿç‰©å­¦åˆç†æ€§

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""

# æ³¨é‡Šåˆ†ææç¤ºè¯æ¨¡æ¿
ANNOTATION_ANALYSIS_PROMPT = """è¯·åˆ†æä»¥ä¸‹åŸºå› æ³¨é‡Šç»“æœï¼š

## æ³¨é‡ŠåŸºæœ¬ä¿¡æ¯
- æ³¨é‡Šå·¥å…·: {annotator}
- åŸºå› ç»„é•¿åº¦: {genome_length} bp
- ç‰©ç§ç±»å‹: {kingdom}
- é—ä¼ å¯†ç : {genetic_code}

## æ³¨é‡Šç»Ÿè®¡
- æ€»åŸºå› æ•°: {total_genes}
- è›‹ç™½ç¼–ç åŸºå› : {protein_genes}
- tRNAåŸºå› : {trna_genes}
- rRNAåŸºå› : {rrna_genes}
- å…¶ä»–åŸºå› : {other_genes}

## åŸºå› è¦†ç›–
- ç¼–ç åŒºè¦†ç›–ç‡: {coding_coverage}%
- åŸºå› ç»„åˆ©ç”¨ç‡: {genome_utilization}%
- å¹³å‡åŸºå› é•¿åº¦: {avg_gene_length} bp

## é¢„æœŸæ ‡å‡†ï¼ˆçº¿ç²’ä½“åŸºå› ç»„ï¼‰
- é¢„æœŸè›‹ç™½ç¼–ç åŸºå› : 13ä¸ª
- é¢„æœŸtRNAåŸºå› : 22ä¸ª
- é¢„æœŸrRNAåŸºå› : 2ä¸ª
- é¢„æœŸæ€»åŸºå› æ•°: 37ä¸ª

## æ£€æµ‹åˆ°çš„é—®é¢˜
{detected_issues}

è¯·è¾“å‡ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„ JSONï¼š
{{
  "annotation_quality": {{
    "overall_score": 0.0åˆ°1.0ä¹‹é—´çš„æ€»ä½“è´¨é‡è¯„åˆ†,
    "grade": "A/B/C/D/Fç­‰çº§è¯„å®š",
    "summary": "æ³¨é‡Šè´¨é‡æ€»ç»“"
  }},
  "completeness_analysis": {{
    "protein_genes_complete": true/false,
    "trna_genes_complete": true/false,
    "rrna_genes_complete": true/false,
    "missing_genes": ["ç¼ºå¤±åŸºå› åˆ—è¡¨"],
    "extra_genes": ["é¢å¤–åŸºå› åˆ—è¡¨"],
    "completeness_percentage": ç™¾åˆ†æ¯”æ•°å€¼
  }},
  "quality_issues": [
    {{
      "type": "é—®é¢˜ç±»å‹",
      "severity": "low/medium/high/critical",
      "description": "é—®é¢˜æè¿°",
      "affected_genes": ["å½±å“çš„åŸºå› "],
      "suggestion": "ä¿®å¤å»ºè®®"
    }}
  ],
  "functional_analysis": {{
    "essential_pathways_covered": true/false,
    "metabolic_completeness": 0.0åˆ°1.0ä¹‹é—´çš„ä»£è°¢å®Œæ•´æ€§,
    "unusual_features": ["å¼‚å¸¸ç‰¹å¾åˆ—è¡¨"],
    "phylogenetic_consistency": "ä¸ç³»ç»Ÿå‘è‚²çš„ä¸€è‡´æ€§è¯„ä¼°"
  }},
  "curation_recommendations": [
    {{
      "priority": "low/medium/high/critical",
      "action": "å»ºè®®æ“ä½œ",
      "target": "ç›®æ ‡åŸºå› æˆ–åŒºåŸŸ",
      "method": "æ¨èæ–¹æ³•",
      "expected_outcome": "é¢„æœŸç»“æœ"
    }}
  ],
  "validation_strategy": {{
    "experimental_validation_needed": true/false,
    "recommended_experiments": ["æ¨èå®éªŒ"],
    "confidence_level": 0.0åˆ°1.0ä¹‹é—´çš„ç½®ä¿¡åº¦,
    "reliability_factors": ["å¯é æ€§å› ç´ "]
  }},
  "publication_readiness": {{
    "ready": true/false,
    "confidence": 0.0åˆ°1.0ä¹‹é—´çš„ç½®ä¿¡åº¦,
    "required_improvements": ["éœ€è¦æ”¹è¿›çš„æ–¹é¢"],
    "quality_metrics": {{"æŒ‡æ ‡å": æ•°å€¼}}
  }},
  "next_steps": ["ä¸‹ä¸€æ­¥å»ºè®®"],
  "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}}"""


class AnnotationAgent(BaseAgent):
    """åŸºäº AI çš„åŸºå› æ³¨é‡Šæ™ºèƒ½ä½“"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("annotation", config)
        
        # Annotation ç‰¹æœ‰é…ç½®
        self.genetic_code = self.config.get("genetic_code", 2)  # çº¿ç²’ä½“é—ä¼ å¯†ç 
        self.expected_genes = {
            "protein": 13,
            "trna": 22,
            "rrna": 2,
            "total": 37
        }
        self.supported_annotators = ["mitos", "geseq", "cpgavas", "prokka"]
    
    def get_capability(self) -> AgentCapability:
        """è¿”å› Annotation Agent çš„èƒ½åŠ›æè¿°"""
        return AgentCapability(
            name="annotation",
            description="AI-powered gene annotation analysis and quality assessment",
            supported_inputs=["assembly", "kingdom", "genetic_code"],
            supported_outputs=["annotations", "annotation_stats", "quality_report"],
            resource_requirements={
                "cpu_cores": 4,
                "memory_gb": 16,
                "disk_gb": 10,
                "estimated_time_sec": 900
            },
            dependencies=["mitos", "blast", "hmmer"]
        )
    
    def prepare(self, workdir: Path, **kwargs) -> None:
        """å‡†å¤‡é˜¶æ®µ - è®¾ç½®å·¥ä½œç›®å½•å’Œæ£€æŸ¥å·¥å…·"""
        self.workdir = workdir
        self.workdir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ³¨é‡Šåˆ†æç›®å½•
        (self.workdir / "annotation").mkdir(exist_ok=True)
        
        logger.info(f"Annotation Agent prepared in {workdir}")
    
    def run(self, inputs: Dict[str, Any], **config) -> StageResult:
        """è¿è¡Œæ³¨é‡Šåˆ†æ"""
        return self.execute_stage(inputs)
    
    def finalize(self) -> None:
        """æ¸…ç†å’Œåå¤„ç†"""
        logger.info("Annotation Agent finalized")
    
    def validate_inputs(self, inputs: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        required_fields = ["assembly"]
        for field in required_fields:
            if field not in inputs:
                logger.error(f"Missing required input field: {field}")
                return False
        
        # æ£€æŸ¥ç»„è£…æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        assembly_file = inputs.get("assembly")
        if assembly_file and not Path(assembly_file).exists():
            logger.error(f"Assembly file not found: {assembly_file}")
            return False
        
        return True
    
    def _diagnose_annotation_error(self, error_msg: str, stderr_content: str,
                                   stdout_content: str, tool_name: str) -> Dict[str, Any]:
        """
        è¯Šæ–­æ³¨é‡Šé”™è¯¯
        
        Args:
            error_msg: å¼‚å¸¸æ¶ˆæ¯
            stderr_content: æ ‡å‡†é”™è¯¯è¾“å‡º
            stdout_content: æ ‡å‡†è¾“å‡º
            tool_name: å·¥å…·åç§°
        
        Returns:
            è¯Šæ–­ç»“æœï¼ŒåŒ…å«é”™è¯¯ç±»å‹ã€èƒ½å¦ä¿®å¤ã€ä¿®å¤å»ºè®®
        """
        diagnosis_prompt = f"""åˆ†æä»¥ä¸‹åŸºå› æ³¨é‡Šé”™è¯¯ï¼š

å·¥å…·: {tool_name}
å¼‚å¸¸: {error_msg}

æ ‡å‡†é”™è¯¯è¾“å‡ºï¼ˆæœ€å1000å­—ç¬¦ï¼‰:
```
{stderr_content[-1000:] if stderr_content else "æ— "}
```

æ ‡å‡†è¾“å‡ºï¼ˆæœ€å1000å­—ç¬¦ï¼‰:
```
{stdout_content[-1000:] if stdout_content else "æ— "}
```

è¯·è¯Šæ–­ï¼š
1. é”™è¯¯ç±»å‹ï¼štool_not_found/out_of_memory/timeout/parameter_error/sequence_format/tool_bug/unknown
2. æ ¹æœ¬åŸå› ï¼šç®€æ´è¯´æ˜
3. èƒ½å¦è‡ªåŠ¨ä¿®å¤ï¼štrue/false
4. ä¿®å¤å»ºè®®ï¼š
   - å¦‚æœæ˜¯å·¥å…·ä¸å¯ç”¨ï¼šæ¨èå¤‡é€‰å·¥å…·
   - å¦‚æœæ˜¯è¶…æ—¶ï¼šå¢åŠ è¶…æ—¶æ—¶é—´
   - å¦‚æœæ˜¯å‚æ•°é”™è¯¯ï¼šè°ƒæ•´genetic_codeç­‰å‚æ•°
   - å¦‚æœæ˜¯åºåˆ—æ ¼å¼ï¼šæ— æ³•ä¿®å¤

è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "error_type": "ç±»å‹",
  "root_cause": "åŸå› ",
  "can_fix": true/false,
  "fix_strategy": "retry/adjust_params/switch_tool/abort",
  "suggestions": {{
    "alternative_tool": "å·¥å…·åæˆ–null",
    "parameter_adjustments": {{"å‚æ•°": "å€¼"}},
    "explanation": "ä¸ºä»€ä¹ˆè¿™æ ·èƒ½è§£å†³"
  }}
}}"""
        
        try:
            diagnosis = self.generate_llm_json(
                prompt=diagnosis_prompt,
                schema={
                    "type": "object",
                    "properties": {
                        "error_type": {"type": "string"},
                        "root_cause": {"type": "string"},
                        "can_fix": {"type": "boolean"},
                        "fix_strategy": {"type": "string"},
                        "suggestions": {"type": "object"}
                    }
                },
                temperature=0.3
            )
            logger.info(f"ğŸ” Annotation Error diagnosis: {diagnosis['error_type']} - {diagnosis['root_cause']}")
            return diagnosis
        except Exception as e:
            logger.warning(f"AI diagnosis failed: {e}, using rule-based diagnosis")
            return self._rule_based_diagnosis(error_msg, stderr_content)
    
    def _rule_based_diagnosis(self, error_msg: str, stderr: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ç®€å•é”™è¯¯è¯Šæ–­ï¼ˆAI ä¸å¯ç”¨æ—¶çš„å¤‡é€‰ï¼‰"""
        error_lower = (error_msg + " " + stderr).lower()
        
        if "not found" in error_lower or "command not found" in error_lower:
            return {
                "error_type": "tool_not_found",
                "root_cause": "Annotation tool not installed",
                "can_fix": True,
                "fix_strategy": "switch_tool",
                "suggestions": {
                    "alternative_tool": "geseq",
                    "explanation": "Try alternative annotation tool"
                }
            }
        elif "timeout" in error_lower or "timed out" in error_lower:
            return {
                "error_type": "timeout",
                "root_cause": "Annotation timeout",
                "can_fix": True,
                "fix_strategy": "adjust_params",
                "suggestions": {
                    "parameter_adjustments": {"timeout": "increase"},
                    "explanation": "Increase timeout"
                }
            }
        elif "sequence" in error_lower or "fasta" in error_lower or "format" in error_lower:
            return {
                "error_type": "sequence_format",
                "root_cause": "Input sequence format error",
                "can_fix": False,
                "fix_strategy": "abort",
                "suggestions": {
                    "explanation": "Sequence format is invalid"
                }
            }
        else:
            return {
                "error_type": "unknown",
                "root_cause": "Unknown annotation error",
                "can_fix": False,
                "fix_strategy": "abort",
                "suggestions": {}
            }
    
    def _execute_annotation_with_retry(self, inputs: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ‰§è¡Œæ³¨é‡Šï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨ä¿®å¤
        
        è¿™æ˜¯ Agent çš„æ ¸å¿ƒèƒ½åŠ›ï¼šè‡ªå·±å¤„ç†é”™è¯¯ï¼Œè‡ªå·±å°è¯•ä¿®å¤
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            æ³¨é‡Šç»“æœ
        
        Raises:
            RuntimeError: ç¡®å®æ— æ³•ä¿®å¤æ—¶æŠ›å‡º
        """
        retry_count = 0
        current_params = {
            "timeout": self.config.get("timeout", 3600)
        }
        current_tool = inputs.get("annotator", "mitos")
        
        while retry_count <= max_retries:
            try:
                inputs_copy = inputs.copy()
                inputs_copy["annotator"] = current_tool
                inputs_copy.update(current_params)
                
                logger.info(
                    f"ğŸ”§ Annotation attempt {retry_count + 1}/{max_retries + 1} "
                    f"with {current_tool}"
                )
                
                # æ‰§è¡Œæ³¨é‡Š
                result = self.run_annotation(inputs_copy)
                
                logger.info(f"âœ… Annotation succeeded on attempt {retry_count + 1}")
                return result
                
            except Exception as e:
                retry_count += 1
                error_msg = str(e)
                
                logger.warning(f"âŒ Annotation attempt {retry_count} failed: {error_msg}")
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ
                if retry_count > max_retries:
                    logger.error(
                        f"ğŸ’” Annotation failed after {max_retries} retries. "
                        f"Cannot auto-fix. Last error: {error_msg}"
                    )
                    raise RuntimeError(
                        f"Annotation failed after {max_retries} attempts.\n"
                        f"Tool: {current_tool}\n"
                        f"Last error: {error_msg}\n"
                        f"Please check:\n"
                        f"1. Tool installation (mitos, geseq)\n"
                        f"2. Input assembly quality\n"
                        f"3. System resources\n"
                        f"4. Logs in {self.workdir}/annotation/"
                    )
                
                # è¯»å–é”™è¯¯æ—¥å¿—
                stderr_content = ""
                stdout_content = ""
                try:
                    stderr_path = self.workdir / "annotation" / "stderr.log"
                    stdout_path = self.workdir / "annotation" / "stdout.log"
                    if stderr_path.exists():
                        stderr_content = stderr_path.read_text(encoding='utf-8', errors='ignore')
                    if stdout_path.exists():
                        stdout_content = stdout_path.read_text(encoding='utf-8', errors='ignore')
                except Exception:
                    pass
                
                # AI è¯Šæ–­é”™è¯¯
                diagnosis = self._diagnose_annotation_error(
                    error_msg, stderr_content, stdout_content, current_tool
                )
                
                # åˆ¤æ–­èƒ½å¦ä¿®å¤
                if not diagnosis["can_fix"]:
                    logger.error(f"ğŸ’” Error cannot be auto-fixed: {diagnosis['root_cause']}")
                    raise RuntimeError(
                        f"Annotation error cannot be automatically fixed.\n"
                        f"Error type: {diagnosis['error_type']}\n"
                        f"Root cause: {diagnosis['root_cause']}\n"
                        f"Please fix manually and retry."
                    )
                
                # æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤
                fix_strategy = diagnosis["fix_strategy"]
                suggestions = diagnosis.get("suggestions", {})
                
                if fix_strategy == "switch_tool":
                    alt_tool = suggestions.get("alternative_tool")
                    if alt_tool:
                        logger.info(f"ğŸ”„ Switching from {current_tool} to {alt_tool}")
                        logger.info(f"   Reason: {suggestions.get('explanation', 'N/A')}")
                        current_tool = alt_tool
                    else:
                        logger.error("No alternative tool available")
                        continue
                
                elif fix_strategy == "adjust_params":
                    param_adjustments = suggestions.get("parameter_adjustments", {})
                    if param_adjustments:
                        adjusted = self.auto_adjust_parameters(error_msg, current_params)
                        current_params.update(adjusted)
                        logger.info(f"ğŸ”§ Adjusted parameters: {param_adjustments}")
                        logger.info(f"   Reason: {suggestions.get('explanation', 'N/A')}")
                    else:
                        logger.warning("No parameter adjustments suggested, simple retry")
                
                elif fix_strategy == "retry":
                    logger.info("ğŸ”„ Simple retry without changes")
                
                else:
                    logger.warning(f"Unknown fix strategy: {fix_strategy}, aborting")
                    raise RuntimeError(f"Unknown fix strategy: {fix_strategy}")
        
        raise RuntimeError("Unexpected error in retry loop")
    
    def execute_stage(self, inputs: Dict[str, Any]) -> StageResult:
        """æ‰§è¡ŒåŸºå› æ³¨é‡Šé˜¶æ®µ"""
        self.status = AgentStatus.RUNNING
        self.emit_event("stage_start", stage="annotation")
        
        try:
            # éªŒè¯è¾“å…¥
            if not self.validate_inputs(inputs):
                raise ValueError("Input validation failed")
            
            # æ‰§è¡Œæ³¨é‡Šï¼ˆå¸¦æ™ºèƒ½é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰
            annotation_results = self._execute_annotation_with_retry(inputs, max_retries=3)
            
            # AI åˆ†ææ³¨é‡Šç»“æœ
            ai_analysis = self.analyze_annotation_results(annotation_results)
            
            # æ„å»ºç»“æœ
            result = StageResult(
                status=AgentStatus.FINISHED,
                outputs={
                    "annotation_results": annotation_results,
                    "ai_analysis": ai_analysis,
                    "annotation_file": annotation_results.get("annotation_file"),
                    "quality_score": ai_analysis.get("annotation_quality", {}).get("overall_score", 0.7)
                },
                metrics={
                    "total_genes": annotation_results.get("total_genes", 0),
                    "protein_genes": annotation_results.get("protein_genes", 0),
                    "completeness": ai_analysis.get("completeness_analysis", {}).get("completeness_percentage", 0),
                    "quality_score": ai_analysis.get("annotation_quality", {}).get("overall_score", 0.7)
                },
                logs={"annotation_stats": self.workdir / "annotation_stats.json" if self.workdir else Path("annotation_stats.json")}
            )
            
            self.status = AgentStatus.FINISHED
            self.emit_event("stage_complete", stage="annotation", success=True)
            
            return result
            
        except Exception as e:
            logger.error(f"Annotation failed: {e}")
            self.status = AgentStatus.FAILED
            self.emit_event("stage_complete", stage="annotation", success=False, error=str(e))
            
            return StageResult(
                status=AgentStatus.FAILED,
                outputs={},
                metrics={},
                logs={},
                errors=[str(e)]
            )
    
    def run_annotation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå› æ³¨é‡Š"""
        assembly_file = inputs["assembly"]
        kingdom = inputs.get("kingdom", "animal")
        annotator = inputs.get("annotator", "mitos")
        interactive = inputs.get("interactive", False)
        
        # Plant + GeSeqè·¯å¾„
        if kingdom == "plant" and annotator == "geseq":
            if not interactive:
                raise AnnotationFailedError(
                    "Plant annotation with GeSeq requires interactive mode.\n"
                    "GeSeq is a web-based service that needs manual operation.\n\n"
                    "Run with: mito-forge pipeline --kingdom plant --interactive\n\n"
                    "Alternative: Install CPGAVAS2 for local annotation."
                )
            
            # è§¦å‘GeSeqå‘å¯¼
            from ...utils.geseq_guide import GeSeqGuide
            from .exceptions import PipelinePausedException
            
            guide = GeSeqGuide(
                assembly_path=Path(assembly_file),
                kingdom=kingdom,
                workdir=self.workdir or Path(".")
            )
            guide.display_instructions()
            guide.open_browser()
            
            # æŠ›å‡ºæš‚åœå¼‚å¸¸
            raise PipelinePausedException(
                task_id=guide.task_id,
                message=f"Pipeline paused for GeSeq annotation.\n"
                        f"Resume with: mito-forge resume {guide.task_id} --annotation <result.gbk>"
            )
        
        logger.info(f"Running annotation with {annotator} on {assembly_file}")
        
        # å°è¯•è¿è¡ŒçœŸå®æ³¨é‡Šå·¥å…·
        try:
            import shutil
            from pathlib import Path
            
            ann_dir = (self.workdir or Path(".")) / "annotation"
            ann_dir.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦å­˜åœ¨
            def find_tool(tool_name: str) -> str:
                if shutil.which(tool_name):
                    return tool_name
                try:
                    from ...utils.tools_manager import ToolsManager
                    tm = ToolsManager(project_root=Path.cwd())
                    p = tm.where(tool_name)
                    if p:
                        return str(p)
                except Exception:
                    pass
                return None
            
            if annotator.lower() == "mitos":
                # MITOSåªæ”¯æŒåŠ¨ç‰©(Metazoan)çº¿ç²’ä½“,ä¸æ”¯æŒæ¤ç‰©
                if kingdom != "animal":
                    raise ToolNotFoundError(
                        f"MITOS only supports animal mitochondrial genomes.\n"
                        f"For plant annotation, use GeSeq (web-based) in interactive mode:\n"
                        f"  mito-forge pipeline --kingdom plant --interactive\n"
                        f"Or install CPGAVAS2 for local annotation."
                    )
                
                exe = find_tool("runmitos.py") or find_tool("mitos")
                if exe:
                    # MITOS å‚æ•°: --input assembly.fasta --code 2 --outdir output
                    genetic_code = 2  # åŠ¨ç‰©çº¿ç²’ä½“é—ä¼ å¯†ç 
                    args = [
                        "--input", str(assembly_file),
                        "--code", str(genetic_code),
                        "--outdir", str(ann_dir)
                    ]
                    rc = self.run_tool(exe, args, cwd=ann_dir)
                    if rc.get("exit_code") == 0:
                        # è§£æ MITOS è¾“å‡º
                        try:
                            from ...utils.parsers import parse_mitos_output
                            parsed = parse_mitos_output(ann_dir)
                            
                            if parsed['success']:
                                return {
                                    "annotator": "mitos",
                                    "genome_length": 0,  # éœ€è¦ä» assembly_file è·å–
                                    "kingdom": kingdom,
                                    "genetic_code": genetic_code,
                                    "annotation_file": parsed['files'].get('gff', ''),
                                    "total_genes": parsed['metrics'].get('total_genes', 0),
                                    "protein_genes": parsed['metrics'].get('cds_count', 0),
                                    "trna_genes": parsed['metrics'].get('trna_count', 0),
                                    "rrna_genes": parsed['metrics'].get('rrna_count', 0),
                                    "other_genes": 0,
                                    "coding_coverage": 0,  # éœ€è¦è®¡ç®—
                                    "genome_utilization": 0,
                                    "avg_gene_length": 0,
                                    "detected_issues": [],
                                    "gene_details": parsed['metrics'].get('genes', [])
                                }
                            else:
                                logger.warning(f"MITOS parsing failed: {parsed.get('errors')}")
                        except Exception as e:
                            logger.warning(f"Failed to parse MITOS output: {e}")
        except Exception as _e:
            logger.error(f"Annotation tool execution failed: {_e}")
            raise RuntimeError(
                f"Annotation failed with {annotator}. "
                f"Please ensure the tool is installed and accessible. "
                f"Error: {_e}"
            )
        
        # å¦‚æœæ²¡æœ‰è¿”å›ï¼ˆå·¥å…·ä¸æ”¯æŒæˆ–è§£æå¤±è´¥ï¼‰ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise AnnotationFailedError(
            f"Annotation with {annotator} failed - no results returned.\n"
            f"Possible causes:\n"
            f"1. Annotator tool ({annotator}) not properly installed\n"
            f"2. Input assembly file is invalid or empty\n"
            f"3. Tool execution error\n"
            f"Install MITOS: conda install -c bioconda mitos\n"
            f"Check logs: {self.workdir}/annotation/{annotator}.stdout.log"
        )
    
    def analyze_annotation_results(self, annotation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨ AI åˆ†ææ³¨é‡Šç»“æœ"""
        logger.info("Analyzing annotation results with AI...")
        
        # æ£€æŸ¥ annotation_results æ˜¯å¦ä¸º None
        if not annotation_results:
            logger.warning("Annotation results is None, using default values")
            annotation_results = {}
        
        # å‡†å¤‡åˆ†æè¾“å…¥
        analysis_input = {
            "annotator": annotation_results.get("annotator", "unknown"),
            "genome_length": annotation_results.get("genome_length", 0),
            "kingdom": annotation_results.get("kingdom", "animal"),
            "genetic_code": annotation_results.get("genetic_code", 2),
            "total_genes": annotation_results.get("total_genes", 0),
            "protein_genes": annotation_results.get("protein_genes", 0),
            "trna_genes": annotation_results.get("trna_genes", 0),
            "rrna_genes": annotation_results.get("rrna_genes", 0),
            "other_genes": annotation_results.get("other_genes", 0),
            "coding_coverage": annotation_results.get("coding_coverage", 0),
            "genome_utilization": annotation_results.get("genome_utilization", 0),
            "avg_gene_length": annotation_results.get("avg_gene_length", 0),
            "detected_issues": "\n".join(annotation_results.get("detected_issues", []))
        }
        
        # æ„å»ºæç¤ºè¯
        prompt = ANNOTATION_ANALYSIS_PROMPT.format(**analysis_input)
        detail_level = str((self.config or {}).get("detail_level") or os.getenv("MITO_DETAIL_LEVEL", "quick")).lower()
        if detail_level == "detailed":
            extra_guidance = "è¯·è¾“å‡ºå®Œæ•´ä¸”ç»“æ„åŒ–çš„ç»“æœï¼šæ¯ç±»è¦ç‚¹å°½é‡ç»™å‡º3-5æ¡ï¼ŒåŒ…å«å…³é”®é˜ˆå€¼ä¸æ¨èå‚æ•°ï¼Œæ¨ç†è¦ç®€æ´ä½†è¦†ç›–ä¾æ®ã€‚"
        else:
            extra_guidance = "è¯·ä¿æŒç²¾ç®€ï¼šæ¯ç±»è¦ç‚¹ä¸è¶…è¿‡2æ¡ï¼Œä¸€å¥è¯æ€»ç»“ï¼Œæ¨ç†å°½é‡çŸ­ã€‚"
        prompt = f"{prompt}\n\n### è¾“å‡ºé£æ ¼è¦æ±‚\n{extra_guidance}"
        
        # æ³¨å…¥è®°å¿†ä¸ RAGï¼ˆè‡ªåŠ¨æ¢æµ‹ï¼Œå¯ç”¨å³å¯ç”¨ï¼›ä¸å¯ç”¨æ—¶é™é»˜è·³è¿‡ï¼‰
        try:
            tags = ["annotation", analysis_input.get("annotator", "unknown")]
            mem_items = self.memory_query(tags=tags, top_k=3)
            if mem_items:
                mem_lines = ["å†å²æ‘˜è¦:"]
                for it in mem_items[:3]:
                    summ = str(it.get("summary") or it.get("value") or "")
                    if len(summ) > 200:
                        summ = summ[:200] + "..."
                    mem_lines.append(f"- {summ}")
                prompt = prompt + "\n\n" + "\n".join(mem_lines)
        except Exception:
            pass
        try:
            prompt, citations = self.rag_augment(prompt, task=self.current_task, top_k=4)
        except Exception:
            citations = []
        
        # å®šä¹‰ JSON Schema
        schema = {
            "type": "object",
            "required": ["annotation_quality", "completeness_analysis", "quality_issues", "publication_readiness", "reasoning"],
            "properties": {
                "annotation_quality": {"type": "object"},
                "completeness_analysis": {"type": "object"},
                "quality_issues": {"type": "array"},
                "functional_analysis": {"type": "object"},
                "curation_recommendations": {"type": "array"},
                "validation_strategy": {"type": "object"},
                "publication_readiness": {"type": "object"},
                "next_steps": {"type": "array"},
                "reasoning": {"type": "string"}
            }
        }
        
        try:
            # è°ƒç”¨ AI æ¨¡å‹
            # æ ¹æ®åˆ†çº§è°ƒæ•´ç”Ÿæˆå‚æ•°
            temp = 0.1 if detail_level == "quick" else 0.2
            max_tok = 1800 if detail_level == "quick" else 3500
            ai_analysis = self.generate_llm_json(
                prompt=prompt,
                system=ANNOTATION_SYSTEM_PROMPT,
                schema=schema,
                temperature=temp,
                max_tokens=max_tok
            )
            
            # ä¿å­˜ AI åˆ†æç»“æœ
            if self.workdir:
                analysis_file = self.workdir / "annotation_ai_analysis.json"
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(ai_analysis, f, indent=2, ensure_ascii=False)
            
            # å†™é•¿æœŸè®°å¿†ï¼ˆMem0ï¼‰ï¼ŒåŒ…å«æ³¨é‡Šè´¨é‡è¯„ä¼°æ‘˜è¦ä¸å¼•ç”¨ï¼ˆè‹¥æœ‰ï¼‰
            try:
                aq = ai_analysis.get("annotation_quality", {}) if isinstance(ai_analysis, dict) else {}
                summary = aq.get("summary")
                score = aq.get("overall_score")
                grade = aq.get("grade")
                self.memory_write({
                    "agent": "annotation",
                    "task_id": (self.current_task.task_id if self.current_task else "annotation"),
                    "tags": ["annotation", analysis_input.get("annotator","unknown")],
                    "summary": summary or "",
                    "metrics": {"score": score, "grade": grade},
                    "citations": citations if isinstance(citations, list) else [],
                })
            except Exception:
                pass
            
            # æ³¨å…¥ RAG å¼•ç”¨åˆ°åˆ†æç»“æœ
            try:
                if isinstance(ai_analysis, dict) and isinstance(citations, list) and citations:
                    ai_analysis["references"] = citations
            except Exception:
                pass
            return ai_analysis
            
        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            # è¿”å›åŸºç¡€åˆ†æç»“æœå¹¶æ³¨å…¥å¼•ç”¨
            citations = citations if isinstance(citations, list) else []
            basic = self._get_basic_analysis(annotation_results)
            try:
                if citations:
                    basic["references"] = citations
                aq = basic.get("annotation_quality", {}) if isinstance(basic, dict) else {}
                summary = aq.get("summary")
                score = aq.get("overall_score")
                grade = aq.get("grade")
                self.memory_write({
                    "agent": "annotation",
                    "task_id": (self.current_task.task_id if self.current_task else "annotation"),
                    "tags": ["annotation", annotation_results.get("annotator","unknown")],
                    "summary": summary or "",
                    "metrics": {"score": score, "grade": grade},
                    "citations": citations,
                })
            except Exception:
                pass
            return basic
    
    def _get_basic_analysis(self, annotation_results: Dict[str, Any]) -> Dict[str, Any]:
        """å½“ AI åˆ†æå¤±è´¥æ—¶çš„åŸºç¡€åˆ†æ"""
        total_genes = annotation_results.get("total_genes", 0)
        protein_genes = annotation_results.get("protein_genes", 0)
        trna_genes = annotation_results.get("trna_genes", 0)
        rrna_genes = annotation_results.get("rrna_genes", 0)
        
        # è®¡ç®—å®Œæ•´æ€§
        protein_complete = protein_genes >= self.expected_genes["protein"] * 0.9
        trna_complete = trna_genes >= self.expected_genes["trna"] * 0.9
        rrna_complete = rrna_genes >= self.expected_genes["rrna"]
        
        completeness = (
            (protein_genes / self.expected_genes["protein"]) * 0.5 +
            (trna_genes / self.expected_genes["trna"]) * 0.3 +
            (rrna_genes / self.expected_genes["rrna"]) * 0.2
        )
        completeness = min(1.0, completeness) * 100
        
        # åŸºäºè§„åˆ™çš„è´¨é‡è¯„ä¼°
        if completeness >= 95 and protein_complete and trna_complete and rrna_complete:
            grade = "A"
            score = 0.9
        elif completeness >= 85 and protein_complete and (trna_complete or rrna_complete):
            grade = "B"
            score = 0.8
        elif completeness >= 70:
            grade = "C"
            score = 0.6
        else:
            grade = "D"
            score = 0.4
        
        return {
            "annotation_quality": {
                "overall_score": score,
                "grade": grade,
                "summary": f"åŸºäºè§„åˆ™çš„æ³¨é‡Šè¯„ä¼°ï¼šå®Œæ•´æ€§ {completeness:.1f}%, æ€»åŸºå› æ•° {total_genes}"
            },
            "completeness_analysis": {
                "protein_genes_complete": protein_complete,
                "trna_genes_complete": trna_complete,
                "rrna_genes_complete": rrna_complete,
                "missing_genes": [],
                "extra_genes": [],
                "completeness_percentage": completeness
            },
            "quality_issues": [],
            "functional_analysis": {
                "essential_pathways_covered": protein_complete,
                "metabolic_completeness": min(1.0, protein_genes / self.expected_genes["protein"]),
                "unusual_features": [],
                "phylogenetic_consistency": "éœ€è¦è¿›ä¸€æ­¥éªŒè¯"
            },
            "curation_recommendations": [
                {
                    "priority": "medium",
                    "action": "æ‰‹åŠ¨æ£€æŸ¥åŸºå› è¾¹ç•Œ",
                    "target": "æ‰€æœ‰åŸºå› ",
                    "method": "åºåˆ—æ¯”å¯¹éªŒè¯",
                    "expected_outcome": "æé«˜æ³¨é‡Šå‡†ç¡®æ€§"
                }
            ],
            "validation_strategy": {
                "experimental_validation_needed": score < 0.8,
                "recommended_experiments": ["RT-PCRéªŒè¯", "è›‹ç™½è´¨ç»„å­¦"],
                "confidence_level": 0.7,
                "reliability_factors": ["åŸºäºè§„åˆ™çš„ç®€å•è¯„ä¼°"]
            },
            "publication_readiness": {
                "ready": score >= 0.7,
                "confidence": 0.7,
                "required_improvements": ["æ‰‹åŠ¨æ ¡æ­£", "å®éªŒéªŒè¯"] if score < 0.8 else [],
                "quality_metrics": {
                    "completeness": completeness,
                    "gene_count": total_genes
                }
            },
            "next_steps": ["æ‰‹åŠ¨æ ¡æ­£æ³¨é‡Š", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"],
            "reasoning": "ä½¿ç”¨åŸºäºè§„åˆ™çš„å¤‡ç”¨åˆ†ææ–¹æ³•"
        }