"""
è´¨é‡æ§åˆ¶æ™ºèƒ½ä½“ - åŸºäº AI çš„æ•°æ®è´¨é‡åˆ†æ
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional

from .base_agent import BaseAgent
from .types import AgentStatus, StageResult, AgentCapability
from .exceptions import QCFailedError, ToolNotFoundError
from ...utils.logging import get_logger

logger = get_logger(__name__)

# QC åˆ†æç³»ç»Ÿæç¤ºè¯
QC_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦è´¨é‡æ§åˆ¶ä¸“å®¶ï¼Œè´Ÿè´£åˆ†ææµ‹åºæ•°æ®çš„è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

ä½ çš„èŒè´£ï¼š
1. åˆ†ææµ‹åºæ•°æ®çš„è´¨é‡æŒ‡æ ‡ï¼ˆè´¨é‡åˆ†æ•°ã€GCå«é‡ã€é•¿åº¦åˆ†å¸ƒç­‰ï¼‰
2. è¯†åˆ«æ½œåœ¨çš„é—®é¢˜ï¼ˆæ¥å¤´æ±¡æŸ“ã€è´¨é‡ä¸‹é™ã€åå·®ç­‰ï¼‰
3. æ¨èåˆé€‚çš„è´¨é‡æ§åˆ¶å’Œæ•°æ®é¢„å¤„ç†ç­–ç•¥
4. è¯„ä¼°æ•°æ®æ˜¯å¦é€‚åˆåç»­åˆ†æ

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""

# QC åˆ†ææç¤ºè¯æ¨¡æ¿
QC_ANALYSIS_PROMPT = """è¯·åˆ†æä»¥ä¸‹æµ‹åºæ•°æ®çš„è´¨é‡æ§åˆ¶ç»“æœï¼š

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- æ–‡ä»¶å: {filename}
- è¯»é•¿ç±»å‹: {read_type}
- æ€»è¯»æ•°: {total_reads}
- æ€»ç¢±åŸºæ•°: {total_bases}
- å¹³å‡è¯»é•¿: {avg_length}

## è´¨é‡æŒ‡æ ‡
- å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality}
- Q20 ç™¾åˆ†æ¯”: {q20_percent}%
- Q30 ç™¾åˆ†æ¯”: {q30_percent}%
- GC å«é‡: {gc_content}%

## é•¿åº¦åˆ†å¸ƒ
- æœ€çŸ­è¯»é•¿: {min_length}
- æœ€é•¿è¯»é•¿: {max_length}
- N50: {n50}

## æ£€æµ‹åˆ°çš„é—®é¢˜
{detected_issues}

è¯·è¾“å‡ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„ JSONï¼š
{{
  "quality_assessment": {{
    "overall_score": 0.0åˆ°1.0ä¹‹é—´çš„æ€»ä½“è´¨é‡è¯„åˆ†,
    "grade": "A/B/C/D/Fç­‰çº§è¯„å®š",
    "summary": "è´¨é‡è¯„ä¼°æ€»ç»“"
  }},
  "issues_found": [
    {{
      "type": "é—®é¢˜ç±»å‹",
      "severity": "low/medium/high/critical",
      "description": "é—®é¢˜æè¿°",
      "affected_percentage": ç™¾åˆ†æ¯”æ•°å€¼
    }}
  ],
  "recommendations": [
    {{
      "action": "å»ºè®®æ“ä½œ",
      "tool": "æ¨èå·¥å…·",
      "parameters": {{"å‚æ•°å": "å‚æ•°å€¼"}},
      "priority": "low/medium/high",
      "expected_improvement": "é¢„æœŸæ”¹å–„æ•ˆæœ"
    }}
  ],
  "preprocessing_strategy": {{
    "trimming_needed": true/false,
    "filtering_needed": true/false,
    "adapter_removal": true/false,
    "quality_threshold": æ•°å€¼,
    "length_threshold": æ•°å€¼
  }},
  "suitability": {{
    "for_assembly": true/false,
    "for_annotation": true/false,
    "confidence": 0.0åˆ°1.0ä¹‹é—´çš„ç½®ä¿¡åº¦,
    "limiting_factors": ["é™åˆ¶å› ç´ åˆ—è¡¨"]
  }},
  "next_steps": ["ä¸‹ä¸€æ­¥å»ºè®®"],
  "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}}"""


class QCAgent(BaseAgent):
    """åŸºäº AI çš„è´¨é‡æ§åˆ¶æ™ºèƒ½ä½“"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("qc", config)
        
        # QC ç‰¹æœ‰é…ç½®
        self.quality_threshold = self.config.get("quality_threshold", 20)
        self.length_threshold = self.config.get("length_threshold", 100)
        self.supported_tools = ["fastqc", "nanoplot", "trimmomatic", "cutadapt"]
    
    def get_capability(self) -> AgentCapability:
        """è¿”å› QC Agent çš„èƒ½åŠ›æè¿°"""
        return AgentCapability(
            name="qc",
            description="AI-powered quality control analysis for sequencing data",
            supported_inputs=["reads", "read_type"],
            supported_outputs=["qc_report", "quality_metrics", "recommendations"],
            resource_requirements={
                "cpu_cores": 4,
                "memory_gb": 8,
                "disk_gb": 5,
                "estimated_time_sec": 300
            },
            dependencies=["fastqc", "nanoplot"]
        )
    
    def prepare(self, workdir: Path, **kwargs) -> None:
        """å‡†å¤‡é˜¶æ®µ - è®¾ç½®å·¥ä½œç›®å½•å’Œæ£€æŸ¥å·¥å…·"""
        self.workdir = workdir
        self.workdir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»º QC åˆ†æç›®å½•
        (self.workdir / "qc").mkdir(exist_ok=True)
        
        logger.info(f"QC Agent prepared in {workdir}")
    
    def run(self, inputs: Dict[str, Any], **config) -> StageResult:
        """è¿è¡Œ QC åˆ†æ"""
        return self.execute_stage(inputs)
    
    def finalize(self) -> None:
        """æ¸…ç†å’Œåå¤„ç†"""
        logger.info("QC Agent finalized")
    
    def validate_inputs(self, inputs: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        required_fields = ["reads"]
        for field in required_fields:
            if field not in inputs:
                logger.error(f"Missing required input field: {field}")
                return False
        
        # æ£€æŸ¥è¯»å–æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        reads_file = inputs.get("reads")
        if reads_file and not Path(reads_file).exists():
            logger.error(f"Reads file not found: {reads_file}")
            return False
        
        return True
    
    def _diagnose_qc_error(self, error_msg: str, stderr_content: str,
                          stdout_content: str, tool_name: str) -> Dict[str, Any]:
        """
        è¯Šæ–­ QC é”™è¯¯
        
        Args:
            error_msg: å¼‚å¸¸æ¶ˆæ¯
            stderr_content: æ ‡å‡†é”™è¯¯è¾“å‡º
            stdout_content: æ ‡å‡†è¾“å‡º
            tool_name: å·¥å…·åç§°
        
        Returns:
            è¯Šæ–­ç»“æœï¼ŒåŒ…å«é”™è¯¯ç±»å‹ã€èƒ½å¦ä¿®å¤ã€ä¿®å¤å»ºè®®
        """
        diagnosis_prompt = f"""åˆ†æä»¥ä¸‹è´¨é‡æ§åˆ¶é”™è¯¯ï¼š

å·¥å…·: {tool_name}
å¼‚å¸¸: {error_msg}

æ ‡å‡†é”™è¯¯è¾“å‡ºï¼ˆæœ€å1000å­—ç¬¦ï¼‰:
```
{stderr_content[-1000:] if stderr_content else "æ— "}
```

æ ‡å‡†è¾“å‡ºï¼ˆæœ€å1000å­—ç¬¦ï¼‰:
```
{stdout_content[-1000:] if stdout_content else "æ— "}
```

è¯·è¯Šæ–­ï¼š
1. é”™è¯¯ç±»å‹ï¼štool_not_found/out_of_memory/timeout/parameter_error/data_format/tool_bug/unknown
2. æ ¹æœ¬åŸå› ï¼šç®€æ´è¯´æ˜
3. èƒ½å¦è‡ªåŠ¨ä¿®å¤ï¼štrue/false
4. ä¿®å¤å»ºè®®ï¼š
   - å¦‚æœæ˜¯å·¥å…·ä¸å¯ç”¨ï¼šæ¨èå¤‡é€‰å·¥å…·
   - å¦‚æœæ˜¯å†…å­˜ä¸è¶³ï¼šå‡å°‘çº¿ç¨‹
   - å¦‚æœæ˜¯æ ¼å¼é”™è¯¯ï¼šå°è¯•å…¶ä»–å·¥å…·
   - å¦‚æœæ˜¯æ•°æ®é—®é¢˜ï¼šæ— æ³•ä¿®å¤

è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "error_type": "ç±»å‹",
  "root_cause": "åŸå› ",
  "can_fix": true/false,
  "fix_strategy": "retry/adjust_params/switch_tool/abort",
  "suggestions": {{
    "alternative_tool": "å·¥å…·åæˆ–null",
    "parameter_adjustments": {{"å‚æ•°": "å€¼"}},
    "explanation": "ä¸ºä»€ä¹ˆè¿™æ ·èƒ½è§£å†³"
  }}
}}"""
        
        try:
            diagnosis = self.generate_llm_json(
                prompt=diagnosis_prompt,
                schema={
                    "type": "object",
                    "properties": {
                        "error_type": {"type": "string"},
                        "root_cause": {"type": "string"},
                        "can_fix": {"type": "boolean"},
                        "fix_strategy": {"type": "string"},
                        "suggestions": {"type": "object"}
                    }
                },
                temperature=0.3
            )
            logger.info(f"ğŸ” QC Error diagnosis: {diagnosis['error_type']} - {diagnosis['root_cause']}")
            return diagnosis
        except Exception as e:
            logger.warning(f"AI diagnosis failed: {e}, using rule-based diagnosis")
            return self._rule_based_diagnosis(error_msg, stderr_content)
    
    def _rule_based_diagnosis(self, error_msg: str, stderr: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ç®€å•é”™è¯¯è¯Šæ–­ï¼ˆAI ä¸å¯ç”¨æ—¶çš„å¤‡é€‰ï¼‰"""
        error_lower = (error_msg + " " + stderr).lower()
        
        if "not found" in error_lower or "command not found" in error_lower:
            return {
                "error_type": "tool_not_found",
                "root_cause": "QC tool not installed",
                "can_fix": True,
                "fix_strategy": "switch_tool",
                "suggestions": {
                    "alternative_tool": "nanoplot",
                    "explanation": "Try alternative QC tool"
                }
            }
        elif "out of memory" in error_lower or "oom" in error_lower:
            return {
                "error_type": "out_of_memory",
                "root_cause": "Memory exhausted",
                "can_fix": True,
                "fix_strategy": "adjust_params",
                "suggestions": {
                    "parameter_adjustments": {"threads": "reduce_half"},
                    "explanation": "Reduce threads to save memory"
                }
            }
        elif "format" in error_lower or "invalid" in error_lower:
            return {
                "error_type": "data_format",
                "root_cause": "Input format error",
                "can_fix": False,
                "fix_strategy": "abort",
                "suggestions": {
                    "explanation": "Input data format is invalid"
                }
            }
        else:
            return {
                "error_type": "unknown",
                "root_cause": "Unknown QC error",
                "can_fix": False,
                "fix_strategy": "abort",
                "suggestions": {}
            }
    
    def _execute_qc_with_retry(self, inputs: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ‰§è¡Œ QCï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨ä¿®å¤
        
        è¿™æ˜¯ Agent çš„æ ¸å¿ƒèƒ½åŠ›ï¼šè‡ªå·±å¤„ç†é”™è¯¯ï¼Œè‡ªå·±å°è¯•ä¿®å¤
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            QC ç»“æœ
        
        Raises:
            RuntimeError: ç¡®å®æ— æ³•ä¿®å¤æ—¶æŠ›å‡º
        """
        retry_count = 0
        current_params = {
            "threads": int(self.config.get("threads", 4))
        }
        current_tool = inputs.get("qc_tool", "fastqc")
        
        while retry_count <= max_retries:
            try:
                inputs_copy = inputs.copy()
                inputs_copy["qc_tool"] = current_tool
                inputs_copy.update(current_params)
                
                logger.info(
                    f"ğŸ”§ QC attempt {retry_count + 1}/{max_retries + 1} "
                    f"with {current_tool}"
                )
                
                # æ‰§è¡Œ QC
                result = self.run_qc_analysis(inputs_copy)
                
                logger.info(f"âœ… QC succeeded on attempt {retry_count + 1}")
                return result
                
            except Exception as e:
                retry_count += 1
                error_msg = str(e)
                
                logger.warning(f"âŒ QC attempt {retry_count} failed: {error_msg}")
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ
                if retry_count > max_retries:
                    logger.error(
                        f"ğŸ’” QC failed after {max_retries} retries. "
                        f"Cannot auto-fix. Last error: {error_msg}"
                    )
                    raise RuntimeError(
                        f"QC analysis failed after {max_retries} attempts.\n"
                        f"Tool: {current_tool}\n"
                        f"Last error: {error_msg}\n"
                        f"Please check:\n"
                        f"1. Tool installation (fastqc, nanoplot)\n"
                        f"2. Input data format and integrity\n"
                        f"3. System resources\n"
                        f"4. Logs in {self.workdir}/qc/"
                    )
                
                # è¯»å–é”™è¯¯æ—¥å¿—
                stderr_content = ""
                stdout_content = ""
                try:
                    stderr_path = self.workdir / "qc" / "stderr.log"
                    stdout_path = self.workdir / "qc" / "stdout.log"
                    if stderr_path.exists():
                        stderr_content = stderr_path.read_text(encoding='utf-8', errors='ignore')
                    if stdout_path.exists():
                        stdout_content = stdout_path.read_text(encoding='utf-8', errors='ignore')
                except Exception:
                    pass
                
                # AI è¯Šæ–­é”™è¯¯
                diagnosis = self._diagnose_qc_error(
                    error_msg, stderr_content, stdout_content, current_tool
                )
                
                # åˆ¤æ–­èƒ½å¦ä¿®å¤
                if not diagnosis["can_fix"]:
                    logger.error(f"ğŸ’” Error cannot be auto-fixed: {diagnosis['root_cause']}")
                    raise RuntimeError(
                        f"QC error cannot be automatically fixed.\n"
                        f"Error type: {diagnosis['error_type']}\n"
                        f"Root cause: {diagnosis['root_cause']}\n"
                        f"Please fix manually and retry."
                    )
                
                # æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤
                fix_strategy = diagnosis["fix_strategy"]
                suggestions = diagnosis.get("suggestions", {})
                
                if fix_strategy == "switch_tool":
                    alt_tool = suggestions.get("alternative_tool")
                    if alt_tool:
                        logger.info(f"ğŸ”„ Switching from {current_tool} to {alt_tool}")
                        logger.info(f"   Reason: {suggestions.get('explanation', 'N/A')}")
                        current_tool = alt_tool
                    else:
                        logger.error("No alternative tool available")
                        continue
                
                elif fix_strategy == "adjust_params":
                    param_adjustments = suggestions.get("parameter_adjustments", {})
                    if param_adjustments:
                        adjusted = self.auto_adjust_parameters(error_msg, current_params)
                        current_params.update(adjusted)
                        logger.info(f"ğŸ”§ Adjusted parameters: {param_adjustments}")
                        logger.info(f"   Reason: {suggestions.get('explanation', 'N/A')}")
                    else:
                        logger.warning("No parameter adjustments suggested, simple retry")
                
                elif fix_strategy == "retry":
                    logger.info("ğŸ”„ Simple retry without changes")
                
                else:
                    logger.warning(f"Unknown fix strategy: {fix_strategy}, aborting")
                    raise RuntimeError(f"Unknown fix strategy: {fix_strategy}")
        
        raise RuntimeError("Unexpected error in retry loop")
    
    def execute_stage(self, inputs: Dict[str, Any]) -> StageResult:
        """æ‰§è¡Œè´¨é‡æ§åˆ¶åˆ†æé˜¶æ®µ"""
        self.status = AgentStatus.RUNNING
        self.emit_event("stage_start", stage="qc_analysis")
        
        try:
            # éªŒè¯è¾“å…¥
            if not self.validate_inputs(inputs):
                raise ValueError("Input validation failed")
            
            # æ‰§è¡Œ QC åˆ†æï¼ˆå¸¦æ™ºèƒ½é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰
            qc_results = self._execute_qc_with_retry(inputs, max_retries=3)
            
            # AI åˆ†æç»“æœ
            ai_analysis = self.analyze_qc_results(qc_results)
            
            # æ„å»ºç»“æœ
            result = StageResult(
                status=AgentStatus.FINISHED,
                outputs={
                    "qc_results": qc_results,
                    "ai_analysis": ai_analysis,
                    "quality_score": ai_analysis.get("quality_assessment", {}).get("overall_score", 0.7),
                    "reads": inputs["reads"],
                    "reads2": inputs.get("reads2")
                },
                metrics={
                    "quality_score": ai_analysis.get("quality_assessment", {}).get("overall_score", 0.7),
                    "issues_count": len(ai_analysis.get("issues_found", [])),
                    "recommendations_count": len(ai_analysis.get("recommendations", []))
                },
                logs={"qc_report": self.workdir / "qc_results.json" if self.workdir else Path("qc_results.json")}
            )
            
            self.status = AgentStatus.FINISHED
            self.emit_event("stage_complete", stage="qc_analysis", success=True)
            
            return result
            
        except Exception as e:
            logger.error(f"QC analysis failed: {e}")
            self.status = AgentStatus.FAILED
            self.emit_event("stage_complete", stage="qc_analysis", success=False, error=str(e))
            
            return StageResult(
                status=AgentStatus.FAILED,
                outputs={},
                metrics={},
                logs={},
                errors=[str(e)]
            )
    
    def run_qc_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡ŒåŸºç¡€ QC åˆ†æå·¥å…·"""
        reads_file = inputs["reads"]
        reads2_file = inputs.get("reads2")  # åŒç«¯æµ‹åº R2
        read_type = inputs.get("read_type", "illumina")
        
        if reads2_file:
            logger.info(f"Running QC analysis on paired-end data: {reads_file} + {reads2_file}")
        else:
            logger.info(f"Running QC analysis on {reads_file}")
        # ä¼˜å…ˆå°è¯•çœŸå®å·¥å…·ï¼šfastqc æˆ– NanoPlot
        try:
            import shutil
            qc_dir = (self.workdir or Path(".")) / "qc"
            qc_dir.mkdir(parents=True, exist_ok=True)
            # ç®€å•ç­–ç•¥ï¼šIllumina ä¼˜å…ˆ fastqcï¼Œå…¶ä»–ä¼˜å…ˆ NanoPlot
            prefer_fastqc = (str(read_type).lower() == "illumina")
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦å­˜åœ¨ï¼ˆç³»ç»Ÿ PATH æˆ–é¡¹ç›®æœ¬åœ°ï¼‰
            def tool_exists(tool_name: str) -> bool:
                if shutil.which(tool_name):
                    return True
                try:
                    from ...utils.tools_manager import ToolsManager
                    tm = ToolsManager(project_root=Path.cwd())
                    return tm.where(tool_name) is not None
                except Exception:
                    return False
            
            fastqc_exists = tool_exists("fastqc")
            nanoplot_exists = tool_exists("NanoPlot") or tool_exists("nanoplot")
            if self.config.get("dry_run"):
                pass  # run_tool å†…éƒ¨ä¼šå¤„ç†
            if prefer_fastqc and fastqc_exists:
                # è¿è¡Œ FastQC for R1
                rc1 = self.run_tool("fastqc", ["-o", str(qc_dir.absolute()), str(Path(reads_file).absolute())], cwd=qc_dir)
                
                # å¦‚æœæ˜¯åŒç«¯æ•°æ®ï¼Œä¹Ÿå¯¹ R2 è¿è¡Œ QC
                if reads2_file and rc1.get("exit_code") == 0:
                    rc2 = self.run_tool("fastqc", ["-o", str(qc_dir.absolute()), str(Path(reads2_file).absolute())], cwd=qc_dir)
                
                if rc1.get("exit_code") == 0:
                    # è§£æ FastQC è¾“å‡º
                    try:
                        from ...utils.parsers import find_fastqc_output, parse_fastqc_output
                        zip_file_r1 = find_fastqc_output(qc_dir, Path(reads_file))
                        
                        if zip_file_r1 and zip_file_r1.exists():
                            result_r1 = parse_fastqc_output(zip_file_r1)
                            result_r1["read_type"] = read_type
                            
                            # å¦‚æœæœ‰ R2ï¼Œè§£æå¹¶åˆå¹¶
                            if reads2_file:
                                zip_file_r2 = find_fastqc_output(qc_dir, Path(reads2_file))
                                if zip_file_r2 and zip_file_r2.exists():
                                    result_r2 = parse_fastqc_output(zip_file_r2)
                                    # åˆå¹¶ R1 å’Œ R2 çš„æŒ‡æ ‡
                                    from ...utils.paired_end_utils import merge_paired_qc_metrics
                                    if result_r1.get('success') and result_r2.get('success'):
                                        merged = merge_paired_qc_metrics(result_r1['metrics'], result_r2['metrics'])
                                        merged["read_type"] = read_type
                                        return merged
                            
                            # å•ç«¯æˆ– R2 è§£æå¤±è´¥æ—¶è¿”å› R1 ç»“æœ
                            result = result_r1
                            return result
                        else:
                            raise QCFailedError(
                                f"FastQC output not found: {qc_dir}\n"
                                f"FastQC may have failed to execute properly.\n"
                                f"Check logs: {qc_dir}/fastqc.stderr.log\n"
                                f"Ensure FastQC is properly installed: conda install -c bioconda fastqc"
                            )
                    except Exception as e:
                        raise QCFailedError(
                            f"Failed to parse FastQC output: {e}\n"
                            f"Output directory: {qc_dir}\n"
                            f"Check if FastQC completed successfully."
                        )
            if nanoplot_exists:
                exe = "NanoPlot" if shutil.which("NanoPlot") else "nanoplot"
                rc = self.run_tool(exe, ["--fastq", str(reads_file), "-o", str(qc_dir)], cwd=qc_dir)
                if rc.get("exit_code") == 0:
                    # è§£æ NanoPlot è¾“å‡º
                    try:
                        from ...utils.parsers import parse_nanoplot_output
                        parsed = parse_nanoplot_output(qc_dir)
                        
                        if parsed['success']:
                            return {
                                "filename": Path(reads_file).name,
                                "read_type": read_type,
                                "total_reads": parsed['metrics'].get('total_reads', 0),
                                "total_bases": parsed['metrics'].get('total_bases', 0),
                                "avg_length": parsed['metrics'].get('avg_length', 0),
                                "avg_quality": parsed['metrics'].get('avg_quality', 0),
                                "q20_percent": parsed['metrics'].get('q20_percent', 0),
                                "gc_content": parsed['metrics'].get('gc_content', 0),
                                "n50": parsed['metrics'].get('n50', 0),
                                "detected_issues": []
                            }
                        else:
                            logger.warning(f"NanoPlot parsing failed: {parsed.get('errors')}")
                    except Exception as e:
                        logger.warning(f"Failed to parse NanoPlot output: {e}")
        except Exception as _e:
            logger.error(f"QC tool execution failed: {_e}")
            raise RuntimeError(
                f"QC analysis failed. "
                f"Please ensure FastQC/NanoPlot is installed and accessible. "
                f"Error: {_e}"
            )
    
    def analyze_qc_results(self, qc_results: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨ AI åˆ†æ QC ç»“æœ"""
        logger.info("Analyzing QC results with AI...")
        
        # å‡†å¤‡åˆ†æè¾“å…¥
        # æ ¼å¼åŒ– detected_issues
        detected_issues_list = qc_results.get("detected_issues", [])
        if detected_issues_list and isinstance(detected_issues_list[0], dict):
            # æ–°æ ¼å¼ï¼šåˆ—è¡¨of dict
            detected_issues_str = "\n".join([
                f"- {issue.get('type', 'Unknown')}: {issue.get('description', '')} (severity: {issue.get('severity', 'unknown')})"
                for issue in detected_issues_list
            ])
        else:
            # æ—§æ ¼å¼ï¼šåˆ—è¡¨ of str
            detected_issues_str = "\n".join(detected_issues_list)
        
        analysis_input = {
            "filename": qc_results.get("filename", "unknown"),
            "read_type": qc_results.get("read_type", "unknown"),
            "total_reads": qc_results.get("total_reads", 0),
            "total_bases": qc_results.get("total_bases", 0),
            "avg_length": qc_results.get("avg_length", 0),
            "avg_quality": qc_results.get("avg_quality", 0),
            "q20_percent": qc_results.get("q20_percent", 0),
            "q30_percent": qc_results.get("q30_percent", 0),
            "gc_content": qc_results.get("gc_content", 0),
            "min_length": qc_results.get("min_length", 0),
            "max_length": qc_results.get("max_length", 0),
            "n50": qc_results.get("n50", 0),
            "detected_issues": detected_issues_str
        }
        
        # æ„å»ºæç¤ºè¯
        prompt = QC_ANALYSIS_PROMPT.format(**analysis_input)
        detail_level = str((self.config or {}).get("detail_level") or os.getenv("MITO_DETAIL_LEVEL", "quick")).lower()
        if detail_level == "detailed":
            extra_guidance = "è¯·è¾“å‡ºå®Œæ•´ä¸”ç»“æ„åŒ–çš„ç»“æœï¼šæ¯ç±»è¦ç‚¹å°½é‡ç»™å‡º3-5æ¡ï¼ŒåŒ…å«å…³é”®é˜ˆå€¼ä¸æ¨èå‚æ•°ï¼Œæ¨ç†è¦ç®€æ´ä½†è¦†ç›–ä¾æ®ã€‚"
        else:
            extra_guidance = "è¯·ä¿æŒç²¾ç®€ï¼šæ¯ç±»è¦ç‚¹ä¸è¶…è¿‡2æ¡ï¼Œä¸€å¥è¯æ€»ç»“ï¼Œæ¨ç†å°½é‡çŸ­ã€‚"
        prompt = f"{prompt}\n\n### è¾“å‡ºé£æ ¼è¦æ±‚\n{extra_guidance}"
        
        # æ³¨å…¥è®°å¿†ä¸ RAGï¼ˆè‡ªåŠ¨æ¢æµ‹ï¼Œå¯ç”¨å³å¯ç”¨ï¼›ä¸å¯ç”¨æ—¶é™é»˜è·³è¿‡ï¼‰
        try:
            tags = ["qc", analysis_input.get("filename", "unknown")]
            mem_items = self.memory_query(tags=tags, top_k=3)
            if mem_items:
                mem_lines = ["å†å²æ‘˜è¦:"]
                for it in mem_items[:3]:
                    summ = str(it.get("summary") or it.get("value") or "")
                    if len(summ) > 200:
                        summ = summ[:200] + "..."
                    mem_lines.append(f"- {summ}")
                prompt = prompt + "\n\n" + "\n".join(mem_lines)
        except Exception:
            pass
        try:
            prompt, citations = self.rag_augment(prompt, task=self.current_task, top_k=4)
        except Exception:
            citations = []
        
        # å®šä¹‰ JSON Schema
        schema = {
            "type": "object",
            "required": ["quality_assessment", "issues_found", "recommendations", "suitability", "reasoning"],
            "properties": {
                "quality_assessment": {"type": "object"},
                "issues_found": {"type": "array"},
                "recommendations": {"type": "array"},
                "preprocessing_strategy": {"type": "object"},
                "suitability": {"type": "object"},
                "next_steps": {"type": "array"},
                "reasoning": {"type": "string"}
            }
        }
        
        try:
            # è°ƒç”¨ AI æ¨¡å‹
            # æ ¹æ®åˆ†çº§è°ƒæ•´ç”Ÿæˆå‚æ•°
            temp = 0.1 if detail_level == "quick" else 0.2
            max_tok = 1500 if detail_level == "quick" else 3200
            ai_analysis = self.generate_llm_json(
                prompt=prompt,
                system=QC_SYSTEM_PROMPT,
                schema=schema,
                temperature=temp,
                max_tokens=max_tok
            )
            
            # ä¿å­˜ AI åˆ†æç»“æœ
            if self.workdir:
                analysis_file = self.workdir / "qc_ai_analysis.json"
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(ai_analysis, f, indent=2, ensure_ascii=False)
            
            # å†™é•¿æœŸè®°å¿†ï¼ˆMem0ï¼‰ï¼ŒåŒ…å«è´¨é‡è¯„ä¼°æ‘˜è¦ä¸å¼•ç”¨ï¼ˆè‹¥æœ‰ï¼‰
            try:
                qa = ai_analysis.get("quality_assessment", {}) if isinstance(ai_analysis, dict) else {}
                summary = qa.get("summary")
                score = qa.get("overall_score")
                grade = qa.get("grade")
                self.memory_write({
                    "agent": "qc",
                    "task_id": (self.current_task.task_id if self.current_task else "qc"),
                    "tags": ["qc", analysis_input.get("filename","unknown")],
                    "summary": summary or "",
                    "metrics": {"score": score, "grade": grade},
                    "citations": citations if isinstance(citations, list) else [],
                })
            except Exception:
                pass
            
            # æ³¨å…¥ RAG å¼•ç”¨åˆ°åˆ†æç»“æœ
            try:
                if isinstance(ai_analysis, dict) and isinstance(citations, list) and citations:
                    ai_analysis["references"] = citations
            except Exception:
                pass
            return ai_analysis
            
        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            # è¿”å›åŸºç¡€åˆ†æç»“æœå¹¶æ³¨å…¥å¼•ç”¨
            citations = citations if isinstance(citations, list) else []
            basic = self._get_basic_analysis(qc_results)
            try:
                if citations:
                    basic["references"] = citations
                qa = basic.get("quality_assessment", {}) if isinstance(basic, dict) else {}
                summary = qa.get("summary")
                score = qa.get("overall_score")
                grade = qa.get("grade")
                self.memory_write({
                    "agent": "qc",
                    "task_id": (self.current_task.task_id if self.current_task else "qc"),
                    "tags": ["qc", qc_results.get("filename","unknown")],
                    "summary": summary or "",
                    "metrics": {"score": score, "grade": grade},
                    "citations": citations,
                })
            except Exception:
                pass
            return basic
    
    def _get_basic_analysis(self, qc_results: Dict[str, Any]) -> Dict[str, Any]:
        """å½“ AI åˆ†æå¤±è´¥æ—¶çš„åŸºç¡€åˆ†æ"""
        avg_quality = qc_results.get("avg_quality", 0)
        q30_percent = qc_results.get("q30_percent", 0)
        
        # åŸºäºè§„åˆ™çš„è´¨é‡è¯„ä¼°
        if avg_quality >= 30 and q30_percent >= 85:
            grade = "A"
            score = 0.9
        elif avg_quality >= 25 and q30_percent >= 75:
            grade = "B"
            score = 0.8
        elif avg_quality >= 20 and q30_percent >= 60:
            grade = "C"
            score = 0.6
        else:
            grade = "D"
            score = 0.4
        
        return {
            "quality_assessment": {
                "overall_score": score,
                "grade": grade,
                "summary": f"åŸºäºè§„åˆ™çš„è´¨é‡è¯„ä¼°ï¼šå¹³å‡è´¨é‡ {avg_quality}, Q30 {q30_percent}%"
            },
            "issues_found": [],
            "recommendations": [
                {
                    "action": "è´¨é‡è¿‡æ»¤",
                    "tool": "trimmomatic",
                    "parameters": {"quality_threshold": self.quality_threshold},
                    "priority": "medium",
                    "expected_improvement": "æé«˜æ•°æ®è´¨é‡"
                }
            ],
            "preprocessing_strategy": {
                "trimming_needed": avg_quality < 25,
                "filtering_needed": q30_percent < 80,
                "adapter_removal": True,
                "quality_threshold": self.quality_threshold,
                "length_threshold": self.length_threshold
            },
            "suitability": {
                "for_assembly": score >= 0.6,
                "for_annotation": score >= 0.7,
                "confidence": 0.7,
                "limiting_factors": ["åŸºäºè§„åˆ™çš„ç®€å•è¯„ä¼°"]
            },
            "next_steps": ["è¿›è¡Œæ•°æ®é¢„å¤„ç†", "è¿è¡Œç»„è£…åˆ†æ"],
            "reasoning": "ä½¿ç”¨åŸºäºè§„åˆ™çš„å¤‡ç”¨åˆ†ææ–¹æ³•"
        }