"""
LangGraph èŠ‚ç‚¹å®šä¹‰
æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨æµæ°´çº¿ä¸­çš„ä¸€ä¸ªé˜¶æ®µï¼ŒåŒ…å«å…·ä½“çš„æ‰§è¡Œé€»è¾‘
"""
import json
import time
import os
from pathlib import Path
from typing import Dict, Any, Optional, List

from .state import (
    PipelineState, StageOutputs, DataType, Kingdom, RouteDecision,
    start_stage, complete_stage, fail_stage, skip_stage
)
from ..utils.logging import get_logger

logger = get_logger(__name__)

# === å¼•å…¥çœŸå®è¯„ä¼°æ‰€éœ€çš„ Agents ä¸ç±»å‹ï¼ˆæœ€å°ä¾µå…¥ï¼‰ ===
try:
    from ..core.agents.qc_agent import QCAgent
    from ..core.agents.assembly_agent import AssemblyAgent
    from ..core.agents.annotation_agent import AnnotationAgent
    from ..core.agents.supervisor_agent import SupervisorAgent
    from ..core.agents.types import TaskSpec
except Exception:
    QCAgent = AssemblyAgent = AnnotationAgent = SupervisorAgent = None
    TaskSpec = None

def supervisor_node(state: PipelineState) -> PipelineState:
    """
    Supervisor Agent - æ™ºèƒ½åˆ†æè¾“å…¥æ•°æ®å¹¶åˆ¶å®šæœ€ä¼˜æ‰§è¡Œç­–ç•¥
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. æ·±åº¦åˆ†æè¾“å…¥æ•°æ®ç‰¹å¾ï¼ˆè¯»é•¿åˆ†å¸ƒã€è´¨é‡ã€æ•°æ®é‡ï¼‰
    2. æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·é“¾ç»„åˆ
    3. åŠ¨æ€è°ƒæ•´æ‰§è¡Œå‚æ•°å’Œèµ„æºé…ç½®
    4. åˆ¶å®šå®¹é”™å’Œå¤‡ç”¨ç­–ç•¥
    5. ç›‘æ§æ•´ä½“æ‰§è¡Œè¿›åº¦å’Œèµ„æºä½¿ç”¨
    """
    logger.info(f"Supervisor Agent starting analysis for pipeline {state['pipeline_id']}")
    
    # å¼€å§‹ supervisor é˜¶æ®µ
    start_stage(state, "supervisor")
    
    try:
        inputs = state["inputs"]
        config = state["config"]
        workdir = Path(state["workdir"])
        
        # è·å–kingdomå‚æ•°ï¼Œä¼˜å…ˆä»configä¸­è·å–ï¼Œç„¶åä»inputsä¸­è·å–
        kingdom = config.get("kingdom", inputs.get("kingdom", "animal"))
        
        # è‹¥å·²æä¾›æ¥è‡ª selection çš„å·¥å…·è®¡åˆ’ï¼Œåˆ™ä¼˜å…ˆé‡‡ç”¨ï¼Œé™ä½é‡å¤åˆ¤æ–­
        tool_plan = (config or {}).get("tool_plan")
        preselected_tool_chain = None
        if isinstance(tool_plan, dict):
            try:
                # å°† plan æ˜ å°„ä¸º nodes å†…éƒ¨ä½¿ç”¨çš„ tool_chain ç»“æ„
                qc_tool = None
                if isinstance(tool_plan.get("qc"), list) and tool_plan["qc"]:
                    qc_tool = tool_plan["qc"][0]
                elif isinstance(tool_plan.get("qc"), str):
                    qc_tool = tool_plan["qc"]
                assembler_tool = None
                asm = tool_plan.get("assembler") or {}
                if isinstance(asm, dict) and asm.get("name"):
                    assembler_tool = asm["name"]
                elif isinstance(asm, str):
                    assembler_tool = asm
                polishing_tool = None
                pol_list = tool_plan.get("polishers") or []
                if isinstance(pol_list, list) and pol_list:
                    # nodes çš„ç­–ç•¥é‡Œåªä¿ç•™ä¸€ä¸ªæ ‡è¯†ï¼Œå®é™…æŠ›å…‰ç»†èŠ‚åœ¨æ‰§è¡Œé˜¶æ®µç»†åŒ–
                    polishing_tool = pol_list[0]
                preselected_tool_chain = {
                    "qc": qc_tool or "fastqc",
                    "assembly": assembler_tool or "spades",
                    "polishing": polishing_tool,  # å¯ä¸º None
                    "annotation": "mitos",        # ä¿æŒé»˜è®¤ï¼Œåç»­å¯ç”± kingdom ç»†åŒ–
                }
                # å°†è¯¥é“¾è·¯é¢„å…ˆå†™å…¥ configï¼Œä¾›åç»­é˜¶æ®µç›´æ¥ä½¿ç”¨
                state["config"]["tool_chain"] = preselected_tool_chain
            except Exception as _e:
                logger.warning(f"Failed to consume provided tool_plan, will fall back to internal strategy: {_e}")
                preselected_tool_chain = None

        # === ç¬¬ä¸€æ­¥ï¼šæ·±åº¦æ•°æ®åˆ†æ ===
        logger.info("Performing comprehensive data analysis...")
        data_profile = _analyze_input_data_comprehensive(inputs, workdir)
        
        # === ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½ç­–ç•¥é€‰æ‹©æˆ–é‡‡ç”¨é¢„é€‰æ–¹æ¡ˆ ===
        logger.info("Selecting optimal execution strategy...")
        if preselected_tool_chain:
            # ä»¥é¢„é€‰é“¾è·¯æ„é€ ä¸€ä¸ªä¸å†…éƒ¨æ ¼å¼å…¼å®¹çš„â€œå·²é€‰ç­–ç•¥â€
            # åˆå¹¶ tool_plan ä¸­ assembler.params åˆ° parameters.{assembler}
            asm_params = {}
            try:
                _asm = (tool_plan or {}).get("assembler") or {}
                if isinstance(_asm, dict):
                    _p = _asm.get("params") or {}
                    if isinstance(_p, dict):
                        asm_params = _p
            except Exception:
                asm_params = {}
            assembler_name = preselected_tool_chain.get("assembly") or "spades"
            parameters = {}
            if assembler_name and asm_params:
                parameters[assembler_name] = dict(asm_params)

            optimal_strategy = {
                "name": "Preselected_From_ToolPlan",
                "tools": preselected_tool_chain,
                "parameters": parameters,
                "fallbacks": {
                    "assembly": ["flye", "spades", "unicycler"],
                    "annotation": ["mitos", "geseq"]
                },
                "confidence": 0.9,
                "success_probability": 0.9
            }
        else:
            optimal_strategy = _select_optimal_strategy(data_profile, kingdom)
        
        # === ç¬¬ä¸‰æ­¥ï¼šèµ„æºéœ€æ±‚è¯„ä¼° ===
        logger.info("Estimating resource requirements...")
        resource_plan = _estimate_resource_requirements(data_profile, optimal_strategy)
        
        # === ç¬¬å››æ­¥ï¼šåˆ¶å®šæ‰§è¡Œè®¡åˆ’ ===
        logger.info("Creating detailed execution plan...")
        execution_plan = _create_execution_plan(optimal_strategy, resource_plan, config)
        
        # === ç¬¬äº”æ­¥ï¼šè®¾ç½®ç›‘æ§å’Œå®¹é”™ç­–ç•¥ ===
        logger.info("Setting up monitoring and fallback strategies...")
        monitoring_config = _setup_monitoring_strategy(data_profile, optimal_strategy)
        
        # === æ›´æ–°çŠ¶æ€é…ç½® ===
        state["config"].update({
            # æ•°æ®åˆ†æç»“æœ
            "data_profile": data_profile,
            "detected_read_type": data_profile["read_type"].value,
            "estimated_genome_size": data_profile["estimated_genome_size"],
            "data_quality_score": data_profile["quality_score"],
            
            # ç­–ç•¥é…ç½®
            "selected_strategy": optimal_strategy,
            "tool_chain": optimal_strategy["tools"],
            "tool_parameters": optimal_strategy["parameters"],
            "fallback_tools": optimal_strategy["fallbacks"],
            
            # æ‰§è¡Œè®¡åˆ’
            "execution_plan": execution_plan,
            "stage_sequence": execution_plan["stages"],
            "conditional_stages": execution_plan["conditional_stages"],
            
            # èµ„æºé…ç½®
            "resource_plan": resource_plan,
            "estimated_runtime": resource_plan["total_time_estimate"],
            "memory_requirements": resource_plan["memory_per_stage"],
            
            # ç›‘æ§é…ç½®
            "monitoring": monitoring_config,
            "quality_thresholds": monitoring_config["thresholds"],
            "retry_strategies": monitoring_config["retry_strategies"]
        })
        
        # === å‡†å¤‡ Supervisor è¾“å‡º ===
        supervisor_outputs = {
            "files": {
                "analysis_report": str(workdir / "supervisor_analysis.json"),
                "execution_plan": str(workdir / "execution_plan.json"),
                "resource_plan": str(workdir / "resource_plan.json")
            },
            "metrics": {
                "confidence_score": optimal_strategy["confidence"],
                "expected_success_rate": optimal_strategy["success_probability"],
                "estimated_total_time": resource_plan["total_time_estimate"],
                "data_complexity": data_profile["complexity_score"]
            },
            "metadata": {
                "supervisor_version": "2.0.0",
                "analysis_timestamp": time.time(),
                "strategy_name": optimal_strategy["name"]
            },
            "summary": f"Selected {optimal_strategy['name']} strategy with {optimal_strategy['confidence']:.2f} confidence",
            "success": True
        }
        
        # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        _save_supervisor_analysis(workdir, data_profile, optimal_strategy, execution_plan, resource_plan)
        
        # å®Œæˆ supervisor é˜¶æ®µ
        complete_stage(state, "supervisor", supervisor_outputs)
        
        # å†³å®šä¸‹ä¸€æ­¥è·¯ç”±
        next_stage = _determine_next_stage(execution_plan, state)
        state["current_stage"] = next_stage
        state["route"] = RouteDecision.CONTINUE
        
        logger.info(f"Supervisor analysis completed successfully")
        logger.info(f"Strategy: {optimal_strategy['name']} (confidence: {optimal_strategy['confidence']:.2f})")
        logger.info(f"Estimated runtime: {resource_plan['total_time_estimate']:.1f} minutes")
        logger.info(f"Next stage: {next_stage}")
        
        return state
        
    except Exception as e:
        logger.error(f"Supervisor Agent failed: {e}")
        fail_stage(state, "supervisor", str(e))
        state["route"] = RouteDecision.TERMINATE
        return state

def qc_node(state: PipelineState) -> PipelineState:
    """
    è´¨é‡æ§åˆ¶èŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. è¿è¡Œ FastQC åˆ†æ
    2. æ ¹æ®è´¨é‡å†³å®šæ˜¯å¦éœ€è¦æ¸…ç†
    3. å¯é€‰çš„æ¥å¤´å»é™¤å’Œè´¨é‡ä¿®å‰ª
    4. æ”¯æŒåŒç«¯æµ‹åºæ•°æ®ï¼ˆR1 å’Œ R2ï¼‰
    """
    logger.info("Starting QC stage")
    
    try:
        inputs = state["inputs"]
        # è·å– R2ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        reads2 = inputs.get("reads2")
        if reads2:
            logger.info(f"Processing paired-end data: R1={inputs['reads']}, R2={reads2}")
        config = state["config"]
        workdir = Path(state["workdir"])
        
        # åˆ›å»º QC å·¥ä½œç›®å½•
        qc_dir = workdir / "01_qc"
        qc_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡æ‹Ÿ QC æ‰§è¡Œï¼ˆå®é™…ä¼šè°ƒç”¨ FastQC ç­‰å·¥å…·ï¼‰
        qc_results = _run_qc_analysis(inputs, qc_dir, config)
        
        # å¯é€‰ï¼šè°ƒç”¨çœŸå® QC Agent è¿›è¡Œ LLM è¯„ä¼°ï¼ˆæŒ‰åˆ†çº§ quick/detailed/expert è°ƒæ•´æ·±åº¦ï¼‰
        ai_metrics = {}
        ai_file = None
        try:
            if QCAgent and TaskSpec and state["config"].get("enable_llm_eval", True):
                detail_level = os.getenv("MITO_DETAIL_LEVEL", "quick").lower()
                qc_agent = QCAgent(state["config"])
                # åˆè¯„ï¼ˆquick/detailed/expert éƒ½ä¼šè°ƒç”¨ä¸€æ¬¡ï¼‰
                base_cfg = {"read_type": "illumina", "detail_level": detail_level, "llm_depth": 1}
                # å‡†å¤‡ QC inputsï¼ŒåŒ…å« reads2
                qc_inputs = {"reads": str(inputs["reads"])}
                if reads2:
                    qc_inputs["reads2"] = str(reads2)
                
                task = TaskSpec(
                    task_id="qc_pipeline",
                    agent_type="qc",
                    inputs=qc_inputs,
                    config=base_cfg,
                    workdir=qc_dir
                )
                _res = qc_agent.execute_task(task)
                
                # ç«‹å³æ£€æŸ¥Agentæ‰§è¡ŒçŠ¶æ€,é¿å…åç»­ä»£ç æŠ›å‡ºå¼‚å¸¸
                from ..core.agents.types import AgentStatus
                if _res.status == AgentStatus.FAILED:
                    error_msg = str(_res.errors[0]) if _res.errors else "QC analysis failed"
                    logger.error(f"ğŸ›‘ QC Agent failed, terminating pipeline: {error_msg}")
                    fail_stage(state, "qc", error_msg)
                    state["route"] = RouteDecision.TERMINATE
                    return state
                
                _ai = (_res.outputs or {}).get("ai_analysis", {}) or {}
                merged_ai = dict(_ai) if isinstance(_ai, dict) else {"raw": _ai}
                # expert è¿½åŠ å¤æ ¸ä¸€è½®ï¼ˆç¬¬äºŒæ¬¡è°ƒç”¨ï¼‰ï¼Œåˆå¹¶ç»“æœ
                if detail_level == "expert":
                    review_cfg = {"read_type": "illumina", "detail_level": "expert", "llm_depth": 2, "review_of": merged_ai}
                    # å‡†å¤‡ review inputsï¼Œä¹ŸåŒ…å« reads2
                    review_qc_inputs = {"reads": str(inputs["reads"]), "review": True}
                    if reads2:
                        review_qc_inputs["reads2"] = str(reads2)
                    
                    review_task = TaskSpec(
                        task_id="qc_pipeline_review",
                        agent_type="qc",
                        inputs=review_qc_inputs,
                        config=review_cfg,
                        workdir=qc_dir
                    )
                    _res2 = qc_agent.execute_task(review_task)
                    _ai2 = (_res2.outputs or {}).get("ai_analysis", {}) or {}
                    if isinstance(_ai2, dict):
                        merged_ai["review"] = _ai2
                # æå–å…³é”®æŒ‡æ ‡ï¼ˆä¼˜å…ˆç”¨å¤æ ¸ç»“æœçš„è¯„åˆ†/ç­‰çº§/æ‘˜è¦ï¼‰
                _qa = (merged_ai.get("quality_assessment") or merged_ai.get("qc_quality") or {})
                # å›é€€é€»è¾‘ï¼šå¦‚æœå¤æ ¸å±‚çº§æœªæä¾›æ ‡å‡†é”®ï¼Œå°è¯•ä» review ä¸­å–
                if not _qa and isinstance(merged_ai.get("review"), dict):
                    _qa = (merged_ai["review"].get("quality_assessment") or merged_ai["review"].get("qc_quality") or {})
                ai_metrics = {
                    "ai_quality_score": _qa.get("overall_score"),
                    "ai_grade": _qa.get("grade"),
                    "ai_summary": _qa.get("summary")
                }
                # ä¿å­˜è¯„ä¼°æ–‡ä»¶
                ai_file = str(qc_dir / "qc_ai_analysis.json")
                with open(ai_file, "w", encoding="utf-8") as f:
                    json.dump(merged_ai, f, ensure_ascii=False, indent=2)
                    
        except Exception as _e:
            logger.warning(f"QC LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ: {_e}")
        
        # å‡†å¤‡è¾“å‡º
        files_dict = {
            "qc_report": str(qc_dir / "fastqc_report.html"),
            "clean_reads": qc_results.get("clean_reads", inputs["reads"])
        }
        # å¦‚æœæœ‰ R2ï¼Œä¹Ÿæ·»åŠ åˆ°è¾“å‡º
        if reads2:
            files_dict["clean_reads2"] = qc_results.get("clean_reads2", reads2)
        if ai_file:
            files_dict["qc_ai_analysis"] = ai_file
        
        metrics_dict = {
            "qc_score": qc_results["qc_score"],
            "total_reads": qc_results["total_reads"],
            "clean_reads": qc_results["clean_reads_count"]
        }
        # åˆå¹¶ AI è¯„ä¼°æŒ‡æ ‡
        metrics_dict.update({k: v for k, v in ai_metrics.items() if v is not None})
        
        outputs = StageOutputs(
            files=files_dict,
            metrics=metrics_dict,
            metadata={
                "tool": "fastqc",
                "version": "0.12.1"
            }
        )
        
        # æ›´æ–°çŠ¶æ€
        complete_stage(state, "qc", outputs)
        state["current_stage"] = "assembly"
        state["route"] = RouteDecision.CONTINUE
        
        logger.info(f"QC completed with score: {qc_results['qc_score']}")
        
        return state
        
    except Exception as e:
        logger.error(f"QC failed: {e}")
        fail_stage(state, "qc", str(e))
        # QC Agent å†…éƒ¨å·²ç»å¤„ç†äº†é”™è¯¯ï¼ˆåŒ…æ‹¬é‡è¯•å’Œä¿®å¤ï¼‰
        # å¦‚æœåˆ°è¿™é‡Œè¯´æ˜ç¡®å®æ— æ³•ä¿®å¤ï¼Œç›´æ¥ç»ˆæ­¢
        state["route"] = RouteDecision.TERMINATE
        return state

def assembly_node(state: PipelineState) -> PipelineState:
    """
    ç»„è£…èŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. æ ¹æ®ç­–ç•¥é€‰æ‹©ç»„è£…å·¥å…·ï¼ˆFlye/SPAdes/Unicyclerç­‰ï¼‰
    2. æ‰§è¡ŒåŸºå› ç»„ç»„è£…
    3. ç­›é€‰çº¿ç²’ä½“å€™é€‰åºåˆ—
    4. ç¯åŒ–æ£€æµ‹
    """
    logger.info("Starting Assembly stage")
    
    try:
        config = state["config"]
        workdir = Path(state["workdir"])
        
        # è·å– QC åçš„æ•°æ®
        qc_outputs = state["stage_outputs"].get("qc", {})
        reads_file = qc_outputs.get("files", {}).get("clean_reads", state["inputs"]["reads"])
        reads2_file = qc_outputs.get("files", {}).get("clean_reads2", state["inputs"].get("reads2"))
        
        # è‹¥æ¸…æ´—åçš„readsæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™å›é€€åˆ°åŸå§‹reads
        try:
            if not Path(reads_file).exists():
                logger.warning(f"Clean reads not found at {reads_file}, falling back to original input reads")
                reads_file = state["inputs"]["reads"]
                reads2_file = state["inputs"].get("reads2")
        except Exception as _e:
            logger.warning(f"Failed to validate clean_reads path ({reads_file}), fallback to original reads: {_e}")
            reads_file = state["inputs"]["reads"]
            reads2_file = state["inputs"].get("reads2")
        
        # åˆ›å»ºç»„è£…å·¥ä½œç›®å½•
        assembly_dir = workdir / "02_assembly"
        assembly_dir.mkdir(parents=True, exist_ok=True)
        
        # é€‰æ‹©ç»„è£…å·¥å…·
        tool_chain = config["tool_chain"]
        assembler = tool_chain["assembly"]
        
        # åˆå§‹åŒ–ä¸ºNone,åç»­ä»Agentè·å–çœŸå®ç»“æœ
        assembly_results = None
        mito_candidates = None
        
        # å¯é€‰ï¼šè°ƒç”¨çœŸå® Assembly Agent è¿›è¡Œ LLM è¯„ä¼°ï¼ˆæŒ‰åˆ†çº§ quick/detailed/expert è°ƒæ•´æ·±åº¦ï¼‰
        asm_ai_metrics = {}
        asm_ai_file = None
        try:
            if AssemblyAgent and TaskSpec and state["config"].get("enable_llm_eval", True):
                detail_level = os.getenv("MITO_DETAIL_LEVEL", "quick").lower()
                asm_agent = AssemblyAgent(state["config"])
                detected_rt = state["config"].get("detected_read_type", "illumina")
                # åˆè¯„ä¸€æ¬¡ï¼ˆæ‰€æœ‰åˆ†çº§éƒ½ä¼šæ‰§è¡Œï¼‰
                base_cfg = {"assembler": assembler, "detail_level": detail_level, "llm_depth": 1}
                # å‡†å¤‡ inputsï¼ŒåŒ…å« reads2
                agent_inputs = {
                    "reads": str(reads_file),
                    "read_type": detected_rt,
                    "kingdom": state["config"].get("kingdom", "animal"),
                    "assembler": assembler
                }
                if reads2_file:
                    agent_inputs["reads2"] = str(reads2_file)
                
                task = TaskSpec(
                    task_id="assembly_pipeline",
                    agent_type="assembly",
                    inputs=agent_inputs,
                    config=base_cfg,
                    workdir=assembly_dir
                )
                _res = asm_agent.execute_task(task)
                
                # ç«‹å³æ£€æŸ¥Agentæ‰§è¡ŒçŠ¶æ€
                from ..core.agents.types import AgentStatus
                if _res.status == AgentStatus.FAILED:
                    error_msg = str(_res.errors[0]) if _res.errors else "Assembly failed"
                    logger.error(f"ğŸ›‘ Assembly Agent failed, terminating pipeline: {error_msg}")
                    fail_stage(state, "assembly", error_msg)
                    state["route"] = RouteDecision.TERMINATE
                    return state
                
                # æå–Agentçš„çœŸå®ç»„è£…ç»“æœ
                agent_outputs = _res.outputs or {}
                assembly_results = agent_outputs.get("assembly_results", {})
                mito_file = agent_outputs.get("assembly_file")
                
                # å¦‚æœAgentè¿”å›äº†çº¿ç²’ä½“æ–‡ä»¶,è®¾ç½®mito_candidates
                if mito_file and Path(mito_file).exists():
                    mito_candidates = {
                        "fasta": str(mito_file),
                        "count": 1,  # TODO: ä»assembly_resultsè§£æ
                        "is_circular": assembly_results.get("is_circular", False)
                    }
                
                _ai = agent_outputs.get("ai_analysis", {}) or {}
                merged_ai = dict(_ai) if isinstance(_ai, dict) else {"raw": _ai}
                # expert è¿½åŠ ä¸€è½®å¤æ ¸
                if detail_level == "expert":
                    review_cfg = {"assembler": assembler, "detail_level": "expert", "llm_depth": 2, "review_of": merged_ai}
                    # å‡†å¤‡ review inputsï¼Œä¹ŸåŒ…å« reads2
                    review_inputs = {
                        "reads": str(reads_file),
                        "read_type": detected_rt,
                        "kingdom": state["config"].get("kingdom", "animal"),
                        "assembler": assembler,
                        "review": True
                    }
                    if reads2_file:
                        review_inputs["reads2"] = str(reads2_file)
                    
                    review_task = TaskSpec(
                        task_id="assembly_pipeline_review",
                        agent_type="assembly",
                        inputs=review_inputs,
                        config=review_cfg,
                        workdir=assembly_dir
                    )
                    _res2 = asm_agent.execute_task(review_task)
                    _ai2 = (_res2.outputs or {}).get("ai_analysis", {}) or {}
                    if isinstance(_ai2, dict):
                        merged_ai["review"] = _ai2
                _aq = (merged_ai.get("assembly_quality") or merged_ai.get("assembly_assessment") or {})
                if not _aq and isinstance(merged_ai.get("review"), dict):
                    _aq = (merged_ai["review"].get("assembly_quality") or merged_ai["review"].get("assembly_assessment") or {})
                asm_ai_metrics = {
                    "ai_quality_score": _aq.get("overall_score"),
                    "ai_grade": _aq.get("grade"),
                    "ai_summary": _aq.get("summary")
                }
                asm_ai_file = str(assembly_dir / "assembly_ai_analysis.json")
                with open(asm_ai_file, "w", encoding="utf-8") as f:
                    json.dump(merged_ai, f, ensure_ascii=False, indent=2)
                    
        except Exception as _e:
            logger.warning(f"Assembly Agentæ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ: {_e}")
        
        # Fallback: å¦‚æœAgentæ²¡æœ‰è¿”å›ç»“æœ,ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if assembly_results is None or mito_candidates is None:
            logger.warning("Assembly Agentæœªè¿”å›ç»“æœ,ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            # æ‰§è¡Œç»„è£…(æ¨¡æ‹Ÿ)
            assembly_results = _run_assembly(reads_file, assembly_dir, assembler, config)
            # çº¿ç²’ä½“åºåˆ—ç­›é€‰(æ¨¡æ‹Ÿ)
            mito_candidates = _select_mitochondrial_contigs(
                assembly_results["contigs"],
                config.get("kingdom", "animal")
            )
        
        # å‡†å¤‡è¾“å‡º
        files_dict = {
            "contigs": str(assembly_results["contigs"]),
            "mito_candidates": str(mito_candidates["fasta"]),
            "assembly_stats": str(assembly_dir / "stats.json")
        }
        if asm_ai_file:
            files_dict["assembly_ai_analysis"] = asm_ai_file
        
        metrics_dict = {
            "n50": assembly_results.get("n50", 0),
            "total_contigs": assembly_results.get("num_contigs", assembly_results.get("total_contigs", 0)),
            "mito_candidates_count": mito_candidates.get("count", 0),
            "largest_contig": assembly_results.get("largest_contig", assembly_results.get("max_length", 0)),
            "is_circular": mito_candidates.get("is_circular", False)
        }
        metrics_dict.update({k: v for k, v in asm_ai_metrics.items() if v is not None})
        
        outputs = StageOutputs(
            files=files_dict,
            metrics=metrics_dict,
            metadata={
                "tool": assembler,
                "version": assembly_results.get("version", "unknown")
            }
        )
        
        # æ›´æ–°çŠ¶æ€
        complete_stage(state, "assembly", outputs)
        # ä¸ç›´æ¥è®¾ç½® current_stageï¼Œè®©è·¯ç”±å†³ç­–å™¨å†³å®šä¸‹ä¸€æ­¥ï¼ˆpolish æˆ– annotationï¼‰
        state["route"] = RouteDecision.CONTINUE
        
        logger.info(f"Assembly completed: {mito_candidates['count']} mitochondrial candidates found")
        
        return state
        
    except Exception as e:
        logger.error(f"Assembly failed: {e}")
        fail_stage(state, "assembly", str(e))
        # Assembly Agent å†…éƒ¨å·²ç»å¤„ç†äº†é”™è¯¯ï¼ˆåŒ…æ‹¬é‡è¯•å’Œä¿®å¤ï¼‰
        # å¦‚æœåˆ°è¿™é‡Œè¯´æ˜ç¡®å®æ— æ³•ä¿®å¤ï¼Œç›´æ¥ç»ˆæ­¢
        state["route"] = RouteDecision.TERMINATE
        return state

def annotation_node(state: PipelineState) -> PipelineState:
    """
    æ³¨é‡ŠèŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. å¯¹çº¿ç²’ä½“å€™é€‰åºåˆ—è¿›è¡ŒåŸºå› æ³¨é‡Š
    2. ç”Ÿæˆ GFF å’ŒåŠŸèƒ½æ³¨é‡Š
    3. éªŒè¯æ³¨é‡Šè´¨é‡
    """
    logger.info("Starting Annotation stage")
    
    try:
        config = state["config"]
        workdir = Path(state["workdir"])
        
        # è·å–ç»„è£…ç»“æœ
        assembly_outputs = state["stage_outputs"]["assembly"]
        mito_fasta = assembly_outputs["files"]["mito_candidates"]
        
        # åˆ›å»ºæ³¨é‡Šå·¥ä½œç›®å½•
        annotation_dir = workdir / "03_annotation"
        annotation_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œæ³¨é‡Š
        annotation_results = _run_annotation(mito_fasta, annotation_dir, config)
        
        # å¯é€‰ï¼šè°ƒç”¨çœŸå® Annotation Agent è¿›è¡Œ LLM è¯„ä¼°ï¼ˆæŒ‰åˆ†çº§ quick/detailed/expert è°ƒæ•´æ·±åº¦ï¼‰
        ann_ai_metrics = {}
        ann_ai_file = None
        try:
            if AnnotationAgent and TaskSpec and state["config"].get("enable_llm_eval", True):
                detail_level = os.getenv("MITO_DETAIL_LEVEL", "quick").lower()
                ann_agent = AnnotationAgent(state["config"])
                base_cfg = {"annotator": "mitos", "detail_level": detail_level, "llm_depth": 1}
                task = TaskSpec(
                    task_id="annotation_pipeline",
                    agent_type="annotation",
                    inputs={"assembly": str(mito_fasta), "kingdom": state["config"].get("kingdom", "animal"), "genetic_code": config.get("genetic_code", 2), "interactive": config.get("interactive", False)},
                    config=base_cfg,
                    workdir=annotation_dir
                )
                _res = ann_agent.execute_task(task)
                
                # ç«‹å³æ£€æŸ¥Agentæ‰§è¡ŒçŠ¶æ€
                from ..core.agents.types import AgentStatus
                if _res.status == AgentStatus.FAILED:
                    error_msg = str(_res.errors[0]) if _res.errors else "Annotation failed"
                    logger.error(f"ğŸ›‘ Annotation Agent failed, terminating pipeline: {error_msg}")
                    fail_stage(state, "annotation", error_msg)
                    state["route"] = RouteDecision.TERMINATE
                    return state
                
                _ai = (_res.outputs or {}).get("ai_analysis", {}) or {}
                merged_ai = dict(_ai) if isinstance(_ai, dict) else {"raw": _ai}
                # expert å†è¿›è¡Œä¸€è½®å¤æ ¸
                if detail_level == "expert":
                    review_cfg = {"annotator": "mitos", "detail_level": "expert", "llm_depth": 2, "review_of": merged_ai}
                    review_task = TaskSpec(
                        task_id="annotation_pipeline_review",
                        agent_type="annotation",
                        inputs={"assembly": str(mito_fasta), "kingdom": state["config"].get("kingdom", "animal"), "genetic_code": config.get("genetic_code", 2), "interactive": config.get("interactive", False), "review": True},
                        config=review_cfg,
                        workdir=annotation_dir
                    )
                    _res2 = ann_agent.execute_task(review_task)
                    _ai2 = (_res2.outputs or {}).get("ai_analysis", {}) or {}
                    if isinstance(_ai2, dict):
                        merged_ai["review"] = _ai2
                _aq = (merged_ai.get("annotation_quality") or merged_ai.get("annotation_assessment") or {})
                if not _aq and isinstance(merged_ai.get("review"), dict):
                    _aq = (merged_ai["review"].get("annotation_quality") or merged_ai["review"].get("annotation_assessment") or {})
                ann_ai_metrics = {
                    "ai_quality_score": _aq.get("overall_score"),
                    "ai_grade": _aq.get("grade"),
                    "ai_summary": _aq.get("summary")
                }
                ann_ai_file = str(annotation_dir / "annotation_ai_analysis.json")
                with open(ann_ai_file, "w", encoding="utf-8") as f:
                    json.dump(merged_ai, f, ensure_ascii=False, indent=2)
                    
        except Exception as _e:
            logger.warning(f"Annotation LLMè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ: {_e}")
        
        # å‡†å¤‡è¾“å‡º
        files_dict = {
            "gff": str(annotation_results["gff"]),
            "genbank": str(annotation_results["genbank"]),
            "annotation_table": str(annotation_results["table"])
        }
        if ann_ai_file:
            files_dict["annotation_ai_analysis"] = ann_ai_file
        
        metrics_dict = {
            "genes_found": annotation_results["gene_count"],
            "trna_count": annotation_results["trna_count"],
            "rrna_count": annotation_results["rrna_count"],
            "annotation_completeness": annotation_results["completeness"]
        }
        metrics_dict.update({k: v for k, v in ann_ai_metrics.items() if v is not None})
        
        outputs = StageOutputs(
            files=files_dict,
            metrics=metrics_dict,
            metadata={
                "tool": "mitos",
                "genetic_code": config.get("genetic_code", 2)
            }
        )
        
        # æ›´æ–°çŠ¶æ€
        complete_stage(state, "annotation", outputs)
        state["current_stage"] = "report"
        state["route"] = RouteDecision.CONTINUE
        
        logger.info(f"Annotation completed: {annotation_results['gene_count']} genes found")
        
        return state
        
    except Exception as e:
        logger.error(f"Annotation failed: {e}")
        fail_stage(state, "annotation", str(e))
        # Annotation Agent å†…éƒ¨å·²ç»å¤„ç†äº†é”™è¯¯ï¼ˆåŒ…æ‹¬é‡è¯•å’Œä¿®å¤ï¼‰
        # å¦‚æœåˆ°è¿™é‡Œè¯´æ˜ç¡®å®æ— æ³•ä¿®å¤ï¼Œæ³¨é‡Šå¤±è´¥å¯ä»¥ç»§ç»­æŠ¥å‘Šé˜¶æ®µ
        state["route"] = RouteDecision.CONTINUE
        return state

def polish_node(state: PipelineState) -> PipelineState:
    """
    æŠ›å…‰èŠ‚ç‚¹ - æé«˜ç»„è£…å‡†ç¡®æ€§
    
    èŒè´£ï¼š
    1. æ ¹æ®æ•°æ®ç±»å‹å’Œç­–ç•¥é€‰æ‹©æŠ›å…‰å·¥å…·
    2. æ‰§è¡Œåºåˆ—æŠ›å…‰ï¼ˆRacon/Pilon/Medakaï¼‰
    3. è´¨é‡å¯¹æ¯”ï¼ˆæŠ›å…‰å‰åï¼‰
    4. æ›´æ–°ç»„è£…ç»“æœ
    
    æŠ›å…‰ç­–ç•¥ï¼š
    - Illumina: Pilonï¼ˆéœ€è¦æ¯”å¯¹ï¼‰
    - Nanopore: Racon + Medakaï¼ˆæ¨èï¼‰
    - PacBio CLR: Racon å¤šè½®
    - PacBio HiFi: é€šå¸¸ä¸éœ€è¦æŠ›å…‰
    """
    logger.info("Starting Polishing stage")
    
    start_stage(state, "polish")
    
    try:
        config = state["config"]
        workdir = Path(state["workdir"])
        
        # è·å–ç»„è£…ç»“æœ
        assembly_outputs = state["stage_outputs"].get("assembly", {})
        assembly_file = assembly_outputs.get("files", {}).get("assembly")
        
        if not assembly_file or not Path(assembly_file).exists():
            logger.error("Assembly file not found, skipping polishing")
            skip_stage(state, "polish", "no_assembly_file")
            state["route"] = RouteDecision.CONTINUE
            return state
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æŠ›å…‰
        tool_chain = config.get("tool_chain", {})
        polishing_tool = tool_chain.get("polishing")
        
        if not polishing_tool:
            logger.info("No polishing tool specified, skipping polishing")
            skip_stage(state, "polish", "not_needed")
            state["route"] = RouteDecision.CONTINUE
            return state
        
        # åˆ›å»ºæŠ›å…‰å·¥ä½œç›®å½•
        polish_dir = workdir / "03_polish"
        polish_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–åŸå§‹è¯»æ®µ
        qc_outputs = state["stage_outputs"].get("qc", {})
        reads_file = qc_outputs.get("files", {}).get("clean_reads", state["inputs"]["reads"])
        reads2_file = qc_outputs.get("files", {}).get("clean_reads2", state["inputs"].get("reads2"))
        
        # è·å–æ•°æ®ç±»å‹
        read_type = config.get("detected_read_type", "illumina")
        
        # æ‰§è¡ŒæŠ›å…‰
        logger.info(f"Running {polishing_tool} polishing on {read_type} data")
        
        polish_results = _run_polishing(
            reads_file=reads_file,
            reads2_file=reads2_file,
            assembly_file=assembly_file,
            output_dir=polish_dir,
            tool=polishing_tool,
            read_type=read_type,
            threads=config.get("threads", 4)
        )
        
        # æ ‡è®°å®Œæˆ
        files_dict = {
            "polished_assembly": polish_results["polished_file"],
            "original_assembly": assembly_file
        }
        
        metrics_dict = {
            "tool": polishing_tool,
            "iterations": polish_results.get("iterations", 1),
            "improvement": _calculate_improvement(
                assembly_file, 
                polish_results["polished_file"]
            )
        }
        
        complete_stage(
            state,
            "polish",
            files=files_dict,
            metrics=metrics_dict
        )
        
        # æ›´æ–°ç»„è£…æ–‡ä»¶ä¸ºæŠ›å…‰åçš„ç‰ˆæœ¬
        state["stage_outputs"]["assembly"]["files"]["assembly"] = polish_results["polished_file"]
        
        state["route"] = RouteDecision.CONTINUE
        logger.info(f"Polishing completed with {polishing_tool}")
        
        return state
        
    except Exception as e:
        logger.error(f"Polishing failed: {e}")
        fail_stage(state, "polish", str(e))
        # æŠ›å…‰å¤±è´¥ä¸è‡´å‘½ï¼Œç»§ç»­åç»­æµç¨‹
        state["route"] = RouteDecision.CONTINUE
        return state

def report_node(state: PipelineState) -> PipelineState:
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. æ±‡æ€»æ‰€æœ‰é˜¶æ®µçš„ç»“æœ
    2. ç”Ÿæˆ HTML æŠ¥å‘Š
    3. åˆ›å»ºç»“æœæ‘˜è¦
    """
    logger.info("Generating final report")
    
    try:
        workdir = Path(state["workdir"])
        report_dir = workdir / "report"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_results = _generate_report(state, report_dir)
        
        # æ ‡è®°å®Œæˆ
        state["done"] = True
        state["end_time"] = time.time()
        state["route"] = RouteDecision.CONTINUE  # END ä¼šç”± graph å¤„ç†
        
        logger.info(f"Pipeline completed successfully. Report: {report_results['html']}")
        
        return state
        
    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        state["errors"].append(f"report: {str(e)}")
        state["done"] = True  # å³ä½¿æŠ¥å‘Šå¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå®Œæˆ
        return state

# === Supervisor Agent æ ¸å¿ƒåˆ†æå‡½æ•° ===

def _analyze_input_data_comprehensive(inputs: Dict[str, Any], workdir: Path) -> Dict[str, Any]:
    """
    æ·±åº¦åˆ†æè¾“å…¥æ•°æ®ç‰¹å¾
    
    åˆ†æå†…å®¹ï¼š
    1. è¯»é•¿ç±»å‹å’Œåˆ†å¸ƒ
    2. æ•°æ®è´¨é‡è¯„ä¼°
    3. æ•°æ®é‡å’Œè¦†ç›–åº¦ä¼°ç®—
    4. å¤æ‚åº¦è¯„åˆ†
    """
    reads_path = inputs["reads"]
    
    # åŸºç¡€æ–‡ä»¶ä¿¡æ¯
    file_info = _get_file_info(reads_path)
    
    # è¯»é•¿ç±»å‹æ£€æµ‹
    read_type = _detect_read_type_advanced(reads_path)
    
    # å¿«é€Ÿè´¨é‡è¯„ä¼°
    quality_metrics = _quick_quality_assessment(reads_path)
    
    # æ•°æ®é‡åˆ†æ
    data_stats = _analyze_data_statistics(reads_path, read_type)
    
    # å¤æ‚åº¦è¯„åˆ†
    complexity_score = _calculate_data_complexity(quality_metrics, data_stats, read_type)
    
    return {
        "read_type": read_type,
        "file_info": file_info,
        "quality_metrics": quality_metrics,
        "data_statistics": data_stats,
        "complexity_score": complexity_score,
        "estimated_genome_size": data_stats["estimated_genome_size"],
        "estimated_coverage": data_stats["estimated_coverage"],
        "quality_score": quality_metrics["overall_quality"],
        "analysis_timestamp": time.time()
    }

def _detect_read_type_advanced(reads_path: str) -> DataType:
    """
    é«˜çº§è¯»é•¿ç±»å‹æ£€æµ‹
    
    æ£€æµ‹æ–¹æ³•ï¼š
    1. æ–‡ä»¶åæ¨¡å¼åŒ¹é…
    2. è¯»é•¿åˆ†å¸ƒåˆ†æ
    3. è´¨é‡åˆ†æ•°åˆ†å¸ƒ
    """
    reads_path_lower = reads_path.lower()
    
    # æ–‡ä»¶åæ¨¡å¼æ£€æµ‹
    if any(pattern in reads_path_lower for pattern in ["nanopore", "ont", "minion", "gridion", "promethion"]):
        return DataType.NANOPORE
    elif any(pattern in reads_path_lower for pattern in ["pacbio", "pb", "hifi", "ccs"]):
        return DataType.PACBIO_HIFI
    elif any(pattern in reads_path_lower for pattern in ["clr", "subreads"]):
        return DataType.PACBIO_CLR
    elif any(pattern in reads_path_lower for pattern in ["illumina", "hiseq", "novaseq", "miseq"]):
        return DataType.ILLUMINA
    
    # å¦‚æœæ–‡ä»¶åæ— æ³•ç¡®å®šï¼Œè¿›è¡Œå†…å®¹åˆ†æ
    try:
        read_lengths = _sample_read_lengths(reads_path, sample_size=1000)
        avg_length = sum(read_lengths) / len(read_lengths) if read_lengths else 0
        
        if avg_length > 5000:  # é•¿è¯»é•¿
            return DataType.NANOPORE  # é»˜è®¤ä¸º Nanopore
        elif avg_length > 1000:  # ä¸­ç­‰è¯»é•¿
            return DataType.PACBIO_HIFI
        else:  # çŸ­è¯»é•¿
            return DataType.ILLUMINA
            
    except Exception as e:
        logger.warning(f"Could not analyze read lengths: {e}")
        return DataType.ILLUMINA  # é»˜è®¤å€¼

def _select_optimal_strategy(data_profile: Dict[str, Any], kingdom: Kingdom) -> Dict[str, Any]:
    """
    åŸºäºæ•°æ®ç‰¹å¾é€‰æ‹©æœ€ä¼˜ç­–ç•¥
    
    ç­–ç•¥é€‰æ‹©è€ƒè™‘å› ç´ ï¼š
    1. è¯»é•¿ç±»å‹å’Œè´¨é‡
    2. æ•°æ®é‡å’Œè¦†ç›–åº¦
    3. ç‰©ç§ç±»å‹
    4. å¤æ‚åº¦è¯„åˆ†
    """
    read_type = data_profile["read_type"]
    quality_score = data_profile["quality_score"]
    coverage = data_profile["estimated_coverage"]
    complexity = data_profile["complexity_score"]
    
    # ç­–ç•¥æ•°æ®åº“
    strategy_matrix = {
        # (read_type, kingdom): strategy_config
        (DataType.NANOPORE, Kingdom.ANIMAL): {
            "name": "Nanopore_Animal_Optimized",
            "tools": {
                "qc": "nanoplot",
                "assembly": "flye" if coverage > 30 else "miniasm",
                "polishing": "medaka",
                "annotation": "mitos"
            },
            "parameters": {
                "flye": {"--genome-size": "16k", "--iterations": 2 if quality_score > 0.8 else 1},
                "medaka": {"--model": "r941_min_high_g360"},
                "mitos": {"--genetic-code": 2, "--kingdom": "animal"}
            },
            "fallbacks": {
                "assembly": ["miniasm", "canu"],
                "annotation": ["geseq", "cpgavas"]
            },
            "confidence": min(0.95, 0.7 + quality_score * 0.25),
            "success_probability": 0.92 if quality_score > 0.7 else 0.85
        },
        
        (DataType.ILLUMINA, Kingdom.ANIMAL): {
            "name": "Illumina_Animal_Optimized", 
            "tools": {
                "qc": "fastqc",
                "trimming": "trimmomatic" if quality_score < 0.8 else None,
                "assembly": "spades",
                "annotation": "mitos"
            },
            "parameters": {
                "spades": {"--careful": True, "-k": "21,33,55,77"},
                "trimmomatic": {"LEADING": 3, "TRAILING": 3, "SLIDINGWINDOW": "4:15"},
                "mitos": {"--genetic-code": 2, "--kingdom": "animal"}
            },
            "fallbacks": {
                "assembly": ["unicycler", "abyss"],
                "annotation": ["geseq", "cpgavas"]
            },
            "confidence": min(0.90, 0.75 + quality_score * 0.15),
            "success_probability": 0.88 if coverage > 50 else 0.82
        },
        
        (DataType.PACBIO_HIFI, Kingdom.ANIMAL): {
            "name": "PacBio_HiFi_Animal_Optimized",
            "tools": {
                "qc": "pbccs_qc",
                "assembly": "hifiasm",
                "annotation": "mitos"
            },
            "parameters": {
                "hifiasm": {"-l": 0, "--primary": True},
                "mitos": {"--genetic-code": 2, "--kingdom": "animal"}
            },
            "fallbacks": {
                "assembly": ["flye", "canu"],
                "annotation": ["geseq", "cpgavas"]
            },
            "confidence": min(0.98, 0.85 + quality_score * 0.13),
            "success_probability": 0.95 if quality_score > 0.9 else 0.90
        },
        
        # æ¤ç‰©ç­–ç•¥
        (DataType.NANOPORE, Kingdom.PLANT): {
            "name": "Nanopore_Plant_Optimized",
            "tools": {
                "qc": "nanoplot",
                "assembly": "flye",
                "polishing": "medaka",
                "annotation": "geseq"
            },
            "parameters": {
                "flye": {"--genome-size": "200k", "--iterations": 3},  # æ¤ç‰©çº¿ç²’ä½“æ›´å¤§
                "geseq": {"--kingdom": "plant", "--genetic-code": 1}
            },
            "fallbacks": {
                "assembly": ["canu", "miniasm"],
                "annotation": ["cpgavas", "mitos"]
            },
            "confidence": min(0.88, 0.65 + quality_score * 0.23),
            "success_probability": 0.85 if quality_score > 0.7 else 0.78
        }
    }
    
    # è·å–åŸºç¡€ç­–ç•¥
    strategy_key = (read_type, kingdom)
    base_strategy = strategy_matrix.get(strategy_key)
    
    if not base_strategy:
        # å›é€€åˆ°é»˜è®¤ç­–ç•¥
        logger.warning(f"No specific strategy for {read_type} + {kingdom}, using default")
        base_strategy = strategy_matrix[(DataType.ILLUMINA, Kingdom.ANIMAL)]
        base_strategy["confidence"] *= 0.8  # é™ä½ç½®ä¿¡åº¦
    
    # æ ¹æ®æ•°æ®ç‰¹å¾åŠ¨æ€è°ƒæ•´ç­–ç•¥
    adjusted_strategy = _adjust_strategy_by_data(base_strategy, data_profile)
    
    return adjusted_strategy

def _adjust_strategy_by_data(strategy: Dict[str, Any], data_profile: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¹æ®æ•°æ®ç‰¹å¾åŠ¨æ€è°ƒæ•´ç­–ç•¥"""
    adjusted = strategy.copy()
    
    quality_score = data_profile["quality_score"]
    coverage = data_profile["estimated_coverage"]
    complexity = data_profile["complexity_score"]
    
    # ä½è´¨é‡æ•°æ®è°ƒæ•´
    if quality_score < 0.6:
        adjusted["confidence"] *= 0.8
        # å¢åŠ è´¨æ§æ­¥éª¤
        if "trimming" not in adjusted["tools"]:
            adjusted["tools"]["trimming"] = "trimmomatic"
        # è°ƒæ•´ç»„è£…å‚æ•°
        if "spades" in adjusted["tools"]["assembly"]:
            adjusted["parameters"]["spades"]["--careful"] = True
    
    # ä½è¦†ç›–åº¦è°ƒæ•´
    if coverage < 20:
        adjusted["confidence"] *= 0.9
        adjusted["success_probability"] *= 0.9
        # ä½¿ç”¨æ›´ä¿å®ˆçš„ç»„è£…å‚æ•°
        if "flye" in adjusted["tools"]["assembly"]:
            adjusted["parameters"]["flye"]["--iterations"] = 1
    
    # é«˜å¤æ‚åº¦æ•°æ®è°ƒæ•´
    if complexity > 0.8:
        adjusted["confidence"] *= 0.85
        # å¢åŠ å¤‡ç”¨å·¥å…·
        if len(adjusted["fallbacks"]["assembly"]) < 3:
            adjusted["fallbacks"]["assembly"].append("canu")
    
    return adjusted

def _estimate_resource_requirements(data_profile: Dict[str, Any], strategy: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä¼°ç®—èµ„æºéœ€æ±‚
    
    è€ƒè™‘å› ç´ ï¼š
    1. æ•°æ®å¤§å°
    2. é€‰æ‹©çš„å·¥å…·
    3. é¢„æœŸå¤æ‚åº¦
    """
    file_size_gb = data_profile["file_info"]["size_gb"]
    read_type = data_profile["read_type"]
    coverage = data_profile["estimated_coverage"]
    
    # åŸºç¡€èµ„æºéœ€æ±‚ï¼ˆæ ¹æ®å·¥å…·å’Œæ•°æ®ç±»å‹ï¼‰
    base_requirements = {
        DataType.NANOPORE: {
            "memory_per_gb": 4,  # GB memory per GB data
            "time_per_gb": 15,   # minutes per GB data
            "cpu_cores": 8
        },
        DataType.ILLUMINA: {
            "memory_per_gb": 6,
            "time_per_gb": 25,
            "cpu_cores": 16
        },
        DataType.PACBIO_HIFI: {
            "memory_per_gb": 3,
            "time_per_gb": 10,
            "cpu_cores": 12
        }
    }
    
    base_req = base_requirements[read_type]
    
    # è®¡ç®—å„é˜¶æ®µèµ„æºéœ€æ±‚
    stage_requirements = {
        "qc": {
            "memory_gb": max(2, file_size_gb * 0.5),
            "time_minutes": max(5, file_size_gb * 2),
            "cpu_cores": 4
        },
        "assembly": {
            "memory_gb": max(8, file_size_gb * base_req["memory_per_gb"]),
            "time_minutes": max(30, file_size_gb * base_req["time_per_gb"]),
            "cpu_cores": base_req["cpu_cores"]
        },
        "annotation": {
            "memory_gb": max(4, file_size_gb * 1),
            "time_minutes": max(10, coverage * 0.5),
            "cpu_cores": 4
        },
        "report": {
            "memory_gb": 2,
            "time_minutes": 5,
            "cpu_cores": 2
        }
    }
    
    # è®¡ç®—æ€»éœ€æ±‚
    total_memory = max(stage_requirements[stage]["memory_gb"] for stage in stage_requirements)
    total_time = sum(stage_requirements[stage]["time_minutes"] for stage in stage_requirements)
    
    return {
        "memory_per_stage": stage_requirements,
        "peak_memory_gb": total_memory,
        "total_time_estimate": total_time,
        "recommended_cpu_cores": base_req["cpu_cores"],
        "disk_space_gb": file_size_gb * 5,  # 5x for intermediate files
        "resource_efficiency_score": min(1.0, 10 / max(file_size_gb, 1e-9))  # æ•ˆç‡è¯„åˆ†ï¼ˆé˜²æ­¢é™¤é›¶ï¼‰
    }

def _create_execution_plan(strategy: Dict[str, Any], resource_plan: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’"""
    
    # åŸºç¡€é˜¶æ®µåºåˆ—
    base_stages = ["qc", "assembly", "annotation", "report"]
    
    # æ ¹æ®é…ç½®è°ƒæ•´é˜¶æ®µ
    stages = []
    conditional_stages = {}
    
    for stage in base_stages:
        if stage == "qc" and config.get("skip_qc", False):
            conditional_stages["qc"] = "skipped_by_config"
            continue
        elif stage == "annotation" and config.get("skip_annotation", False):
            conditional_stages["annotation"] = "skipped_by_config"
            continue
        elif stage == "report" and not config.get("generate_report", True):
            conditional_stages["report"] = "skipped_by_config"
            continue
        
        stages.append(stage)
    
    # æ·»åŠ æ¡ä»¶æ€§é˜¶æ®µ
    if strategy["tools"].get("trimming"):
        conditional_stages["trimming"] = "if_quality_low"
    
    if strategy["tools"].get("polishing"):
        conditional_stages["polishing"] = "after_assembly"
    
    return {
        "stages": stages,
        "conditional_stages": conditional_stages,
        "stage_dependencies": {
            "assembly": ["qc"],
            "annotation": ["assembly"],
            "report": ["annotation"]
        },
        "parallel_stages": [],  # å¯å¹¶è¡Œæ‰§è¡Œçš„é˜¶æ®µ
        "critical_path": stages,  # å…³é”®è·¯å¾„
        "estimated_checkpoints": [f"{stage}_complete" for stage in stages]
    }

def _setup_monitoring_strategy(data_profile: Dict[str, Any], strategy: Dict[str, Any]) -> Dict[str, Any]:
    """è®¾ç½®ç›‘æ§å’Œå®¹é”™ç­–ç•¥"""
    
    quality_score = data_profile["quality_score"]
    complexity = data_profile["complexity_score"]
    
    # è´¨é‡é˜ˆå€¼è®¾ç½®
    thresholds = {
        "qc_min_score": 0.6 if quality_score > 0.7 else 0.4,
        "assembly_min_n50": 10000,
        "assembly_min_contigs": 1,
        "annotation_min_genes": 10,
        "max_memory_usage": 0.9,  # 90% of available memory
        "max_runtime_multiplier": 2.0  # 2x estimated time
    }
    
    # é‡è¯•ç­–ç•¥
    retry_strategies = {
        "qc": {
            "max_retries": 2,
            "retry_conditions": ["tool_error", "timeout"],
            "fallback_tools": ["fastqc", "nanoplot"]
        },
        "assembly": {
            "max_retries": 1,  # ç»„è£…é‡è¯•æˆæœ¬é«˜
            "retry_conditions": ["memory_error", "no_output"],
            "fallback_tools": strategy["fallbacks"]["assembly"]
        },
        "annotation": {
            "max_retries": 3,
            "retry_conditions": ["tool_error", "no_genes_found"],
            "fallback_tools": strategy["fallbacks"]["annotation"]
        }
    }
    
    return {
        "thresholds": thresholds,
        "retry_strategies": retry_strategies,
        "monitoring_interval": 30,  # seconds
        "resource_check_interval": 60,  # seconds
        "checkpoint_frequency": "per_stage",
        "alert_conditions": [
            "memory_usage > 95%",
            "runtime > 150% estimated",
            "disk_space < 1GB"
        ]
    }

def _determine_next_stage(execution_plan: Dict[str, Any], state: PipelineState) -> str:
    """ç¡®å®šä¸‹ä¸€ä¸ªæ‰§è¡Œé˜¶æ®µ"""
    stages = execution_plan["stages"]
    completed = set(state["completed_stages"])
    
    for stage in stages:
        if stage not in completed:
            return stage
    
    return "END"

def _save_supervisor_analysis(
    workdir: Path, 
    data_profile: Dict[str, Any], 
    strategy: Dict[str, Any], 
    execution_plan: Dict[str, Any], 
    resource_plan: Dict[str, Any]
) -> None:
    """ä¿å­˜ Supervisor åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    
    # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
    workdir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºåˆ†ææŠ¥å‘Š
    analysis_report = {
        "supervisor_analysis": {
            "timestamp": time.time(),
            "data_profile": data_profile,
            "selected_strategy": strategy,
            "execution_plan": execution_plan,
            "resource_plan": resource_plan
        }
    }
    
    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    with open(workdir / "supervisor_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, indent=2, default=str, ensure_ascii=False)
    
    with open(workdir / "execution_plan.json", 'w', encoding='utf-8') as f:
        json.dump(execution_plan, f, indent=2, ensure_ascii=False)
    
    with open(workdir / "resource_plan.json", 'w', encoding='utf-8') as f:
        json.dump(resource_plan, f, indent=2, ensure_ascii=False)

# === æ•°æ®åˆ†æè¾…åŠ©å‡½æ•° ===

def _get_file_info(file_path: str) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶åŸºç¡€ä¿¡æ¯"""
    try:
        stat = os.stat(file_path)
        return {
            "path": file_path,
            "size_bytes": stat.st_size,
            "size_gb": stat.st_size / (1024**3),
            "modified_time": stat.st_mtime,
            "exists": True
        }
    except Exception as e:
        return {
            "path": file_path,
            "size_bytes": 0,
            "size_gb": 0,
            "modified_time": 0,
            "exists": False,
            "error": str(e)
        }

def _sample_read_lengths(file_path: str, sample_size: int = 1000) -> List[int]:
    """é‡‡æ ·è¯»é•¿åˆ†å¸ƒ"""
    lengths = []
    try:
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä¼šè§£æ FASTQ/FASTA æ–‡ä»¶
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        if "nanopore" in file_path.lower():
            lengths = [8000 + i * 100 for i in range(sample_size)]
        elif "pacbio" in file_path.lower():
            lengths = [15000 + i * 50 for i in range(sample_size)]
        else:
            lengths = [150 + i for i in range(sample_size)]
    except Exception as e:
        logger.warning(f"Could not sample read lengths: {e}")
    
    return lengths[:sample_size]

def _quick_quality_assessment(file_path: str) -> Dict[str, Any]:
    """å¿«é€Ÿè´¨é‡è¯„ä¼°"""
    # ç®€åŒ–å®ç°ï¼Œå®é™…ä¼šè¿è¡Œ FastQC æˆ–ç±»ä¼¼å·¥å…·
    return {
        "overall_quality": 0.85,  # æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
        "avg_phred_score": 28,
        "gc_content": 0.42,
        "n_content": 0.01,
        "sequence_length_distribution": "normal",
        "adapter_contamination": 0.02
    }

def _analyze_data_statistics(file_path: str, read_type: DataType) -> Dict[str, Any]:
    """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    file_info = _get_file_info(file_path)
    
    # æ ¹æ®è¯»é•¿ç±»å‹ä¼°ç®—
    if read_type == DataType.NANOPORE:
        avg_read_length = 8000
        reads_per_gb = 125000
    elif read_type == DataType.PACBIO_HIFI:
        avg_read_length = 15000
        reads_per_gb = 67000
    else:  # Illumina
        avg_read_length = 150
        reads_per_gb = 6700000
    
    estimated_reads = int(file_info["size_gb"] * reads_per_gb)
    total_bases = estimated_reads * avg_read_length
    
    # çº¿ç²’ä½“åŸºå› ç»„å¤§å°ä¼°ç®—ï¼ˆåŠ¨ç‰©ï¼š16kbï¼Œæ¤ç‰©ï¼š200kbï¼‰
    mito_genome_size = 16000  # é»˜è®¤åŠ¨ç‰©
    estimated_coverage = total_bases / mito_genome_size
    
    return {
        "estimated_reads": estimated_reads,
        "avg_read_length": avg_read_length,
        "total_bases": total_bases,
        "estimated_genome_size": mito_genome_size,
        "estimated_coverage": estimated_coverage,
        "data_density": file_info["size_gb"] / max(1, estimated_reads / 1000000)
    }

def _calculate_data_complexity(quality_metrics: Dict[str, Any], data_stats: Dict[str, Any], read_type: DataType) -> float:
    """è®¡ç®—æ•°æ®å¤æ‚åº¦è¯„åˆ† (0-1)"""
    
    # è´¨é‡å› å­
    quality_factor = quality_metrics["overall_quality"]
    
    # è¦†ç›–åº¦å› å­
    coverage = data_stats["estimated_coverage"]
    coverage_factor = min(1.0, coverage / 50)  # 50x ä¸ºç†æƒ³è¦†ç›–åº¦
    
    # è¯»é•¿ç±»å‹å› å­
    type_factors = {
        DataType.PACBIO_HIFI: 0.2,  # æœ€ç®€å•
        DataType.ILLUMINA: 0.5,     # ä¸­ç­‰
        DataType.NANOPORE: 0.7,     # è¾ƒå¤æ‚
        DataType.PACBIO_CLR: 0.8    # æœ€å¤æ‚
    }
    type_factor = type_factors.get(read_type, 0.5)
    
    # æ•°æ®é‡å› å­
    size_gb = data_stats.get("data_density", 1)
    size_factor = min(1.0, size_gb / 10)  # 10GB ä¸ºå¤æ‚é˜ˆå€¼
    
    # ç»¼åˆå¤æ‚åº¦ (è¶Šé«˜è¶Šå¤æ‚)
    complexity = (
        (1 - quality_factor) * 0.3 +
        (1 - coverage_factor) * 0.2 +
        type_factor * 0.3 +
        size_factor * 0.2
    )
    
    return min(1.0, max(0.0, complexity))

# === å…¼å®¹æ€§å‡½æ•° ===

def _analyze_read_type(inputs: Dict[str, Any]) -> str:
    """åˆ†æè¯»é•¿ç±»å‹ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰"""
    return _detect_read_type_advanced(inputs.get("reads", "")).value

def _select_strategy(read_type: str, kingdom: str) -> Dict[str, Any]:
    """é€‰æ‹©æ‰§è¡Œç­–ç•¥ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰"""
    # è½¬æ¢ä¸ºæ–°çš„æ•°æ®ç±»å‹
    try:
        read_type_enum = DataType(read_type)
        kingdom_enum = Kingdom(kingdom)
    except ValueError:
        read_type_enum = DataType.ILLUMINA
        kingdom_enum = Kingdom.ANIMAL
    
    # åˆ›å»ºç®€åŒ–çš„æ•°æ®é…ç½®æ–‡ä»¶
    mock_profile = {
        "read_type": read_type_enum,
        "quality_score": 0.8,
        "estimated_coverage": 50,
        "complexity_score": 0.5
    }
    
    return _select_optimal_strategy(mock_profile, kingdom_enum)

def _run_qc_analysis(inputs: Dict[str, Any], qc_dir: Path, config: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œ QC åˆ†æï¼ˆæ¨¡æ‹Ÿï¼Œåº”ç”¨ plan å‚æ•°å¹¶è®°å½•ï¼‰"""
    # å®é™…å®ç°ä¼šè°ƒç”¨ FastQC ç­‰å·¥å…·ï¼›è¿™é‡Œè®°å½•å‚æ•°ä»¥ä¾¿æµ‹è¯•éªŒè¯
    try:
        qc_tool = (config.get("tool_chain") or {}).get("qc", "fastqc")
        params = (config.get("tool_parameters") or {}).get(qc_tool, {})
        (qc_dir / "qc_used_params.json").write_text(json.dumps(params, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass
    return {
        "qc_score": 0.92,
        "total_reads": 50000,
        "clean_reads_count": 48500,
        "clean_reads": str(qc_dir / "clean_reads.fastq")
    }

def _run_assembly(reads_file: str, assembly_dir: Path, assembler: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒåŸºå› ç»„ç»„è£…ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…å®ç°ä¼šè°ƒç”¨ Flye/SPAdes ç­‰å·¥å…·
    contigs_file = assembly_dir / "contigs.fasta"
    contigs_file.write_text(">contig_1\nATCGATCGATCG...\n")  # æ¨¡æ‹Ÿåºåˆ—
    
    return {
        "contigs": str(contigs_file),
        "n50": 16800,
        "num_contigs": 3,
        "largest_contig": 16800,
        "version": "2.9.1"
    }

def _select_mitochondrial_contigs(contigs_file: str, kingdom: str) -> Dict[str, Any]:
    """ç­›é€‰çº¿ç²’ä½“åºåˆ—ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…å®ç°ä¼šåŸºäºé•¿åº¦ã€è¦†ç›–åº¦æˆ– BLAST ç­›é€‰
    mito_file = Path(contigs_file).parent / "mitochondrial_candidates.fasta"
    mito_file.write_text(">mito_candidate_1\nATCGATCGATCG...\n")
    
    return {
        "fasta": str(mito_file),
        "count": 1,
        "is_circular": True
    }

def _run_annotation(mito_fasta: str, annotation_dir: Path, config: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒåŸºå› æ³¨é‡Šï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…å®ç°ä¼šè°ƒç”¨ MITOS ç­‰å·¥å…·
    gff_file = annotation_dir / "annotation.gff"
    gff_file.write_text("##gff-version 3\n# Mock annotation\n")
    
    return {
        "gff": str(gff_file),
        "genbank": str(annotation_dir / "annotation.gb"),
        "table": str(annotation_dir / "genes.tsv"),
        "gene_count": 37,
        "trna_count": 22,
        "rrna_count": 2,
        "completeness": 0.95
    }

def _generate_report(state: PipelineState, report_dir: Path) -> Dict[str, Any]:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    from ..reports import generate_html_report
    
    # ç”Ÿæˆ HTML æŠ¥å‘Š
    html_file = report_dir / "pipeline_report.html"
    try:
        generate_html_report(state, html_file)
        logger.info(f"HTML report generated: {html_file}")
    except Exception as e:
        logger.error(f"Failed to generate HTML report: {e}")
        # å›é€€åˆ°ç®€å•æŠ¥å‘Š
        html_file.write_text(f"<html><body><h1>Mito-Forge Pipeline Report</h1><p>Error: {e}</p></body></html>")
    
    # ç”Ÿæˆ JSON æ‘˜è¦
    summary_file = report_dir / "summary.json"
    
    # è®¡ç®— durationï¼Œå¤„ç† None å€¼
    start_time = state.get('start_time', 0)
    end_time = state.get('end_time', 0)
    if start_time and end_time:
        duration = end_time - start_time
    else:
        duration = 0
    
    summary_data = {
        'pipeline_id': state['pipeline_id'],
        'status': 'completed' if state.get('done') else 'failed',
        'start_time': start_time,
        'end_time': end_time,
        'duration': duration,
        'stages': {
            stage: {
                'status': state['stage_status'].get(stage, 'pending'),
                'metrics': state['stage_metrics'].get(stage, {}),
                'files': state['stage_outputs'].get(stage, {}).get('files', {})
            }
            for stage in ['qc', 'assembly', 'polish', 'annotation']
        },
        'config': state.get('config', {}),
        'errors': state.get('errors', [])
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, default=str, ensure_ascii=False)
    
    return {
        "html": str(html_file),
        "summary": str(summary_file)
    }

def _get_fallback_assembler(current_assembler: str) -> str:
    """è·å–å¤‡ç”¨ç»„è£…å·¥å…·"""
    fallbacks = {
        "flye": "spades",
        "spades": "unicycler",
        "unicycler": "flye"
    }
    return fallbacks.get(current_assembler)

def _run_polishing(
    reads_file: str,
    reads2_file: Optional[str],
    assembly_file: str,
    output_dir: Path,
    tool: str,
    read_type: str,
    threads: int = 4
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒæŠ›å…‰
    
    æ ¹æ®å·¥å…·ç±»å‹å’Œæ•°æ®ç±»å‹é€‰æ‹©åˆé€‚çš„æŠ›å…‰ç­–ç•¥
    """
    from ..tools import run_racon, run_pilon, run_medaka
    
    logger.info(f"Running polishing with {tool} on {read_type} data")
    
    reads_path = Path(reads_file)
    reads2_path = Path(reads2_file) if reads2_file else None
    assembly_path = Path(assembly_file)
    
    try:
        if tool.lower() == "racon":
            # Racon ç”¨äºé•¿è¯»æ•°æ®
            minimap_preset = "map-ont" if "nanopore" in read_type.lower() else "map-pb"
            iterations = 3 if "clr" in read_type.lower() else 2
            
            result = run_racon(
                reads=reads_path,
                assembly=assembly_path,
                output_dir=output_dir,
                threads=threads,
                iterations=iterations,
                minimap_preset=minimap_preset
            )
            
        elif tool.lower() == "pilon":
            # Pilon ç”¨äºçŸ­è¯»æ•°æ®
            result = run_pilon(
                reads=reads_path,
                reads2=reads2_path,
                assembly=assembly_path,
                output_dir=output_dir,
                threads=threads,
                memory="16G",
                iterations=1
            )
            
        elif tool.lower() == "medaka":
            # Medaka ç”¨äº Nanopore æ•°æ®
            # æ ¹æ®å®é™…åŒ–å­¦é…æ–¹é€‰æ‹©æ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨é»˜è®¤ï¼‰
            result = run_medaka(
                reads=reads_path,
                assembly=assembly_path,
                output_dir=output_dir,
                model="r941_min_high_g360",
                threads=threads
            )
            
        else:
            raise ValueError(f"Unknown polishing tool: {tool}")
        
        return result
        
    except Exception as e:
        logger.error(f"Polishing with {tool} failed: {e}")
        raise

def _calculate_improvement(original_file: str, polished_file: str) -> Dict[str, Any]:
    """
    è®¡ç®—æŠ›å…‰æ”¹è¿›
    
    å¯¹æ¯”æŠ›å…‰å‰åçš„ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from Bio import SeqIO
        
        # è¯»å–åŸå§‹åºåˆ—
        orig_seqs = list(SeqIO.parse(original_file, "fasta"))
        orig_lengths = [len(seq) for seq in orig_seqs]
        
        # è¯»å–æŠ›å…‰ååºåˆ—
        polish_seqs = list(SeqIO.parse(polished_file, "fasta"))
        polish_lengths = [len(seq) for seq in polish_seqs]
        
        if not orig_lengths or not polish_lengths:
            return {"status": "no_data"}
        
        # è®¡ç®—ç»Ÿè®¡
        orig_total = sum(orig_lengths)
        polish_total = sum(polish_lengths)
        
        orig_n50 = _calculate_n50(orig_lengths)
        polish_n50 = _calculate_n50(polish_lengths)
        
        return {
            "status": "calculated",
            "length_change": polish_total - orig_total,
            "length_change_pct": ((polish_total - orig_total) / orig_total * 100) if orig_total > 0 else 0,
            "n50_change": polish_n50 - orig_n50,
            "n50_change_pct": ((polish_n50 - orig_n50) / orig_n50 * 100) if orig_n50 > 0 else 0,
            "original": {
                "total_length": orig_total,
                "num_contigs": len(orig_lengths),
                "n50": orig_n50
            },
            "polished": {
                "total_length": polish_total,
                "num_contigs": len(polish_lengths),
                "n50": polish_n50
            }
        }
    except Exception as e:
        logger.warning(f"Failed to calculate improvement: {e}")
        return {"status": "error", "message": str(e)}

def _calculate_n50(lengths: list) -> int:
    """è®¡ç®— N50"""
    if not lengths:
        return 0
    
    sorted_lengths = sorted(lengths, reverse=True)
    total = sum(sorted_lengths)
    cumsum = 0
    
    for length in sorted_lengths:
        cumsum += length
        if cumsum >= total / 2:
            return length
    
    return 0